"""
æµç¨‹åœ–
è®€å–è³‡æ–™ â†’ åˆ†å‰²è³‡æ–™ â†’ ç·¨ç¢¼ â†’ å»ºç«‹ Dataset / DataLoader
â†“
å»ºç«‹æ¨¡å‹(BERT+LSTM+CNN)
        â†“
        BERT è¼¸å‡º [batch, seq_len, 768]
        â†“
        BiLSTM  [batch, seq_len, hidden_dim*2]
        â†“
        CNN æ¨¡çµ„ (Conv1D + Dropout + GlobalMaxPooling1D)
        â†“
        Linear åˆ†é¡å™¨(è¼¸å‡ºè©é¨™æ©Ÿç‡)
        â†“
è¨“ç·´æ¨¡å‹(Epochs)
â†“
è©•ä¼°æ¨¡å‹(Accuracy / F1 / Precision / Recall)
â†“
å„²å­˜æ¨¡å‹(.pth)

"""
#å¼•å…¥é‡è¦å¥—ä»¶Import Library
import os
import torch                            #   PyTorch ä¸»æ¨¡çµ„               
import torch.nn as nn                   #	ç¥ç¶“ç¶²è·¯ç›¸é—œçš„å±¤(ä¾‹å¦‚ LSTMã€Linear)        
import pandas as pd
import re

from dotenv import load_dotenv
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset #	æä¾› Datasetã€DataLoader é¡åˆ¥
from transformers import BertTokenizer # BertTokenizeræŠŠæ–‡å­—å¥å­è½‰æ›æˆ BERT æ ¼å¼çš„ token ID,ä¾‹å¦‚ [CLS] ä»Šå¤© å¤©æ°£ ä¸éŒ¯ [SEP] â†’ [101, 1234, 5678, ...]
from sklearn.model_selection import train_test_split
from transformers import BertModel

# ------------------- è¼‰å…¥ .env ç’°å¢ƒè®Šæ•¸ -------------------
load_dotenv()
base_dir = os.getenv("DATA_DIR", "./data")  # å¦‚æœæ²’è¨­ç’°å¢ƒè®Šæ•¸å°±é è¨­ç”¨ ./data

# ------------------- ä½¿ç”¨ç›¸å°è·¯å¾‘æ‰¾ CSV -------------------
#,os.path.join(base_dir, "NorANDScamInfo_data1.csv"),os.path.join(base_dir, "ScamInfo_data1.csv"),os.path.join(base_dir, "NormalInfo_data1.csv")
#å¦‚æœ‰éœ€è¦è¨“ç·´è¤‡æ•¸ç­†è³‡æ–™å¯ä»¥ä½¿ç”¨é€™å€‹æ–¹æ³•csv_files = [os.path.join(base_dir, "æª”æ¡ˆåç¨±1.csv"),os.path.join(base_dir, "æª”æ¡ˆåç¨±2.csv")]
#ç¨‹å¼ç¢¼ä¸€è‡³131è¡Œ

# GPU è¨˜æ†¶é«”é™åˆ¶(å¯é¸)
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:16"

#è³‡æ–™å‰è™•ç†
class BertPreprocessor:
    def __init__(self, tokenizer_name="ckiplab/bert-base-chinese", max_len=128):
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)
        self.max_len = max_len

    def load_and_clean(self, filepath):
        #è¼‰å…¥ CSV ä¸¦æ¸…ç† message æ¬„ä½ã€‚
        df = pd.read_csv(filepath)
        df = df.dropna().drop_duplicates().reset_index(drop=True)
        # æ–‡å­—æ¸…ç†:ç§»é™¤ç©ºç™½ã€ä¿ç•™ä¸­æ–‡è‹±æ•¸èˆ‡æ¨™é»
        df["message"] = df["message"].astype(str)
        df["message"] = df["message"].apply(lambda text: re.sub(r"\s+", "", text))
        df["message"] = df["message"].apply(lambda text: re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚,ï¼ï¼Ÿ]", "", text))
        return df[["message", "label"]]  # ä¿ç•™å¿…è¦æ¬„ä½

    def encode(self, messages):
        #ä½¿ç”¨ HuggingFace BERT Tokenizer å°‡è¨Šæ¯ç·¨ç¢¼æˆBertæ¨¡å‹è¼¸å…¥æ ¼å¼ã€‚
        return self.tokenizer(
            list(messages),
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=self.max_len
        )
#è‡ªå‹•åšè³‡æ–™å‰è™•ç†
def build_bert_inputs(files):
    #å°‡æ­£å¸¸èˆ‡è©é¨™è³‡æ–™åˆ†åˆ¥æŒ‡å®š label,çµ±ä¸€æ¸…ç†ã€ç·¨ç¢¼,å›å‚³æ¨¡å‹å¯ç”¨çš„ input tensors èˆ‡ labelsã€‚
    processor = BertPreprocessor()
    dfs = []
    # åˆä½µæ­£å¸¸ + è©é¨™æª”æ¡ˆæ¸…å–®
    all_files = files

    for filepath in all_files:
        df = processor.load_and_clean(filepath)
        dfs.append(df)
    
    # åˆä½µæ‰€æœ‰è³‡æ–™ã€‚åœ¨è³‡æ–™æ¸…ç†éç¨‹ä¸­dropna():åˆªé™¤æœ‰ç©ºå€¼çš„åˆ—,drop_duplicates():åˆªé™¤é‡è¤‡åˆ—,filter()æˆ–df[...]åšæ¢ä»¶éæ¿¾,concat():å°‡å¤šå€‹ DataFrameåˆä½µ
    # é€™äº›æ“ä½œä¸æœƒè‡ªå‹•é‡æ’ç´¢å¼•,é€ æˆç´¢å¼•äº‚æ‰ã€‚
    # åˆä½µå¾Œçµ±ä¸€ç·¨è™Ÿ(å¸¸è¦‹æ–¼å¤šç­†è³‡æ–™åˆä½µ)all_df = pd.concat(dfs, é—œéµ-->ignore_index=True)
    all_df = pd.concat(dfs, ignore_index=True)
    print(f"âœ… å·²è®€å…¥ {len(all_df)} ç­†è³‡æ–™")
    print(all_df["label"].value_counts())
    #è£½ä½œ train/val è³‡æ–™é›†
    train_texts, val_texts, train_labels, val_labels = train_test_split(
    all_df["message"], all_df["label"],
    stratify=all_df["label"],
    test_size=0.2,
    random_state=25,
    shuffle=True
    )
    # é€²è¡Œ BERT tokenizer ç·¨ç¢¼
    train_inputs = processor.encode(train_texts)
    val_inputs = processor.encode(val_texts)

    return train_inputs, train_labels, val_inputs, val_labels, processor


#å®šç¾© PyTorch Dataset é¡åˆ¥ã€‚ScamDataset ç¹¼æ‰¿è‡ª torch.utils.data.Dataset
#å°‡ BERT è¼¸å‡ºçš„ token èˆ‡å°æ‡‰æ¨™ç±¤å°è£æˆ PyTorch èƒ½ä½¿ç”¨çš„æ ¼å¼
class ScamDataset(Dataset):
    def __init__(self, inputs, labels):
        self.input_ids = inputs["input_ids"]                           # input_ids:å¥å­çš„ token ID;attention_mask:æ³¨æ„åŠ›é®ç½©(0 = padding)
        self.attention_mask = inputs["attention_mask"]                 # token_type_ids:å¥å­çš„ segment å€åˆ†
        self.token_type_ids = inputs["token_type_ids"]                 # torch.tensor(x, dtype=...)å°‡è³‡æ–™(x)è½‰ç‚ºTensorçš„æ¨™æº–åšæ³•ã€‚
        self.labels = torch.tensor(labels.values, dtype=torch.float32) # xå¯ä»¥æ˜¯ listã€NumPy arrayã€pandas series...
# dtypefloat32:æµ®é»æ•¸(å¸¸ç”¨æ–¼ å›æ­¸ æˆ– BCELoss äºŒåˆ†é¡);long:æ•´æ•¸(å¸¸ç”¨æ–¼ å¤šåˆ†é¡ æ­é… CrossEntropyLoss)ã€‚labels.values â†’ è½‰ç‚º NumPy array
    
    def __len__(self):          # å‘Šè¨´ PyTorch é€™å€‹ Dataset æœ‰å¹¾ç­†è³‡æ–™
        return len(self.labels) # çµ¦ len(dataset) æˆ– for i in range(len(dataset)) ç”¨çš„
    
    def __getitem__(self, idx): #æ¯æ¬¡èª¿ç”¨ __getitem__() å›å‚³ä¸€ç­† {input_ids, attention_mask, token_type_ids, labels}
        return {                #DataLoader æ¯æ¬¡æœƒå‘¼å«é€™å€‹æ–¹æ³•å¤šæ¬¡ä¾†æŠ“ä¸€å€‹ batch çš„è³‡æ–™
            "input_ids":self.input_ids[idx],
            "attention_mask":self.attention_mask[idx],
            "token_type_ids":self.token_type_ids[idx],
            "labels":self.labels[idx]
        }

# é€™æ¨£å¯ä»¥åŒæ™‚è™•ç† scam å’Œ normal è³‡æ–™,ä¸ç”¨é‡è¤‡å¯«æ¸…ç†èˆ‡ token è™•ç†
if __name__ == "__main__":
    csv_files = [os.path.join(base_dir, "NorANDScamInfo_data3k.csv")]
    train_inputs, train_labels, val_inputs, val_labels, processor = build_bert_inputs(csv_files)
    
    train_dataset = ScamDataset(train_inputs, train_labels)
    val_dataset = ScamDataset(val_inputs, val_labels)

    # batch_sizeæ¯æ¬¡é€é€²æ¨¡å‹çš„æ˜¯ 8 ç­†è³‡æ–™(è€Œä¸æ˜¯ä¸€ç­†ä¸€ç­†)
    # æ¯æ¬¡å¾ Dataset ä¸­æŠ“ä¸€æ‰¹(batch)è³‡æ–™å‡ºä¾†
    train_loader = DataLoader(train_dataset, batch_size=8)
    val_loader = DataLoader(val_dataset, batch_size=8)

"""
class BertLSTM_CNN_Classifier(nn.Module)è¡¨ç¤º:ä½ å®šç¾©äº†ä¸€å€‹å­é¡åˆ¥,
ç¹¼æ‰¿è‡ª PyTorch çš„åŸºç¤æ¨¡å‹é¡åˆ¥ nn.Moduleã€‚

è‹¥ä½ åœ¨ __init__() è£¡æ²’æœ‰å‘¼å« super().__init__(),
é‚£éº¼çˆ¶é¡åˆ¥ nn.Module çš„åˆå§‹åŒ–é‚è¼¯(åŒ…å«é‡è¦åŠŸèƒ½)å°±ä¸æœƒè¢«åŸ·è¡Œ,
å°è‡´æ•´å€‹æ¨¡å‹é‹ä½œç•°å¸¸æˆ–éŒ¯èª¤ã€‚
"""

# nn.Moduleæ˜¯PyTorchæ‰€æœ‰ç¥ç¶“ç¶²è·¯æ¨¡å‹çš„åŸºç¤é¡åˆ¥,nn.Module æ˜¯ PyTorch æ‰€æœ‰ç¥ç¶“ç¶²è·¯æ¨¡å‹çš„åŸºç¤é¡åˆ¥
class BertLSTM_CNN_Classifier(nn.Module):
    
    def __init__(self, hidden_dim=128, num_layers=1, dropout=0.3):
        
        # super()æ˜¯Pythonæä¾›çš„ä¸€å€‹æ–¹æ³•,ç”¨ä¾†å‘¼å«ã€Œçˆ¶é¡åˆ¥çš„ç‰ˆæœ¬ã€çš„æ–¹æ³•ã€‚
        # å‘¼å«:super().__init__()è®“çˆ¶é¡åˆ¥(nn.Module)è£¡é¢é‚£äº›åŠŸèƒ½ã€å±¬æ€§éƒ½è¢«æ­£ç¢ºåˆå§‹åŒ–ã€‚
        # æ²’super().__init__(),é€™äº›éƒ½ä¸æœƒæ­£ç¢ºé‹ä½œ,æ¨¡å‹æœƒå£æ‰ã€‚
        # super() å°±æ˜¯ Python æä¾›çµ¦ã€Œå­é¡åˆ¥å‘¼å«çˆ¶é¡åˆ¥æ–¹æ³•ã€çš„æ–¹å¼
        super().__init__()
        
        # è¼‰å…¥ä¸­æ–‡é è¨“ç·´çš„ BERT æ¨¡å‹,è¼¸å…¥ç‚ºå¥å­token IDs,è¼¸å‡ºç‚ºæ¯å€‹ token çš„å‘é‡,å¤§å°ç‚º [batch, seq_len, 768]ã€‚
        self.bert = BertModel.from_pretrained("ckiplab/bert-base-chinese") # é€™æ˜¯å¼•å…¥hugging faceä¸­çš„tranceformat
        
        # æ¥æ”¶BERTçš„è¼¸å‡º(768 ç¶­å‘é‡),é€²è¡Œé›™å‘LSTM(BiLSTM)å»ºæ¨¡,è¼¸å‡ºç‚º [batch, seq_len, hidden_dim*2],ä¾‹å¦‚ [batch, seq_len, 256]
        """
        LSTM æ¥æ”¶æ¯å€‹tokençš„768ç¶­å‘é‡(ä¾†è‡ª BERT)ä½œç‚ºè¼¸å…¥,
        é€éæ¯å€‹æ–¹å‘çš„LSTMå£“ç¸®æˆ128ç¶­çš„èªæ„å‘é‡ã€‚
        ç”±æ–¼æ˜¯é›™å‘LSTM,æœƒåŒæ™‚å¾å·¦åˆ°å³(å‰å‘)å’Œå³åˆ°å·¦(å¾Œå‘)å„åšä¸€æ¬¡,
        æœ€å¾Œå°‡å…©å€‹æ–¹å‘çš„è¼¸å‡ºåˆä½µç‚º256ç¶­å‘é‡(128Ã—2)ã€‚
        æ¯æ¬¡è™•ç†ä¸€å€‹ batch(ä¾‹å¦‚ 8 å¥è©±),ä¸€æ¬¡èµ°å®Œæ•´å€‹æ™‚é–“åºåˆ—ã€‚
        """
        self.LSTM = nn.LSTM(input_size=768,        
                            hidden_size=hidden_dim,
                            num_layers=num_layers,
                            batch_first=True,
                            bidirectional=True)
        
         # CNN æ¨¡çµ„:æ¥åœ¨ LSTM å¾Œçš„è¼¸å‡ºä¸Šã€‚å°‡LSTMçš„è¼¸å‡ºè½‰æˆå·ç©å±¤æ ¼å¼,é©ç”¨æ–¼Conv1D,CNNå¯å­¸ç¿’ä½ç½®ä¸è®Šçš„å±€éƒ¨ç‰¹å¾µã€‚
        self.conv1 =  nn.Conv1d(in_channels=hidden_dim*2,
                                out_channels=128,
                                kernel_size=3,  # é€™è£¡kernel_size=3 ç‚º 3-gram ç‰¹å¾µ
                                padding=1)
        
        self.dropout = nn.Dropout(dropout) # éš¨æ©Ÿå°‡éƒ¨åˆ†ç¥ç¶“å…ƒè¨­ç‚º 0,ç”¨ä¾†é˜²æ­¢ overfittingã€‚
        
        self.global_maxpool = nn.AdaptiveAvgPool1d(1)  #å°‡ä¸€æ•´å¥è©±çš„ç‰¹å¾µæ¿ƒç¸®æˆä¸€å€‹å›ºå®šå¤§å°çš„å¥å­è¡¨ç¤ºå‘é‡
        
        # å°‡CNNè¼¸å‡ºçš„128ç¶­ç‰¹å¾µå‘é‡è¼¸å‡ºç‚ºä¸€å€‹ã€Œæ©Ÿç‡å€¼ã€(è©é¨™æˆ–éè©é¨™)ã€‚
        self.classifier = nn.Linear(128,1)
        
    def forward(self, input_ids, attention_mask, token_type_ids):
        #BERT ç·¨ç¢¼
        outputs = self.bert(input_ids=input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids)
        #.last_hidden_stateæ˜¯BertModel.from_pretrained(...)å…§éƒ¨çš„keyï¼Œæœƒè¼¸å‡º    [batch, seq_len, 768]
        hidden_states = outputs.last_hidden_state 

        # é€å…¥ BiLSTM
        # transpose(1, 2) çš„ç”¨é€”æ˜¯ï¼šè®“ LSTM è¼¸å‡ºçš„è³‡æ–™å½¢ç‹€ç¬¦åˆ CNN æ‰€è¦æ±‚çš„æ ¼å¼
        #   å‡è¨­ä½ åŸæœ¬ LSTM è¼¸å‡ºæ˜¯ï¼š    [batch_size, seq_len, hidden_dim*2] = [8, 128, 256]
        # ä½†CNN(Conv1d)çš„è¼¸å…¥æ ¼å¼éœ€è¦æ˜¯ï¼š[batch_size, in_channels, seq_len] = [8, 256, 128]
        # å› æ­¤ä½ éœ€è¦åšï¼š.transpose(1, 2)æŠŠ seq_len å’Œ hidden_dim*2 èª¿æ›
        LSTM_out, _ = self.LSTM(hidden_states)     # [batch, seq_len, hidden_dim*2]
        LSTM_out = LSTM_out.transpose(1, 2)        # [batch, hidden_dim*2, seq_len]

        # å·ç© + Dropout
        x = self.conv1(LSTM_out)                   # [batch, 128, seq_len]
        x = self.dropout(x)
        
        #å…¨å±€æ± åŒ–
        # .squeeze(dim) çš„ä½œç”¨æ˜¯ï¼šæŠŠæŸå€‹ã€Œç¶­åº¦å¤§å°ç‚º 1ã€çš„ç¶­åº¦åˆªæ‰
        # x = self.global_maxpool(x).squeeze(2)   # è¼¸å‡ºæ˜¯ [batch, 128, 1]
        # ä¸ .squeeze(2)ï¼Œä½ æœƒå¾—åˆ° shape ç‚º [batch, 128, 1]ï¼Œä¸æ–¹ä¾¿å¾Œé¢æ¥ Linearã€‚
        # .squeeze(2)=æ‹¿æ‰ç¬¬ 2 ç¶­ï¼ˆæ•¸å€¼æ˜¯ 1ï¼‰ â†’ è®“å½¢ç‹€è®Šæˆ [batch, 128]
        x = self.global_maxpool(x).squeeze(2)      # [batch, 128]

        #åˆ†é¡ & Sigmoid æ©Ÿç‡è¼¸å‡º
        logits = self.classifier(x)
        
        #.sigmoid() â†’ æŠŠ logits è½‰æˆ 0~1 çš„æ©Ÿç‡.squeeze() â†’ è®Šæˆä¸€ç¶­ [batch] é•·åº¦çš„æ©Ÿç‡ list
        """ä¾‹å¦‚ï¼š
logits = [[0.92], [0.05], [0.88], [0.41], ..., [0.17]]
â†’ sigmoid â†’ [[0.715], [0.512], ...]
â†’ squeeze â†’ [0.715, 0.512, ...]
"""
        return torch.sigmoid(logits).squeeze() # æœ€å¾Œè¼¸å‡ºæ˜¯ä¸€å€‹å€¼ä»‹æ–¼ 0 ~ 1 ä¹‹é–“,ä»£è¡¨ã€Œç‚ºè©é¨™è¨Šæ¯çš„æ©Ÿç‡ã€ã€‚
        
# è¨­å®š GPU è£ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åˆå§‹åŒ–æ¨¡å‹
model = BertLSTM_CNN_Classifier().to(device)
# å®šç¾© optimizer å’Œæå¤±å‡½æ•¸
optimizer = torch.optim.Adam(model.parameters(),lr=2e-5)
criterion = nn.BCELoss()

#åªä¿ç•™æ¨è«–å³å¯,æ¨¡å‹è¨“ç·´æ‡‰è©²åœ¨æœ¬åœ°å®Œæˆï¼
if os.path.exists("model.pth"):
    print("âœ… å·²æ‰¾åˆ° model.pth,è¼‰å…¥æ¨¡å‹è·³éè¨“ç·´")
    model.load_state_dict(torch.load("model.pth", map_location=device))
else:
    print("âŒ æœªæ‰¾åˆ° model.pth")

# æœ¬æ©Ÿè¨“ç·´è¿´åœˆ,è¦è¨“ç·´å†å–æ¶ˆè¨»è§£,å¦å‰‡åœ¨ç·šä¸Šç‰ˆæœ¬ä¸€å¾‹è™•æ–¼è¨»è§£ç‹€æ…‹
"""
if __name__ == "__main__": # åªæœ‰ç•¶æˆ‘ã€Œç›´æ¥åŸ·è¡Œé€™å€‹æª”æ¡ˆã€æ™‚,æ‰åŸ·è¡Œä»¥ä¸‹è¨“ç·´ç¨‹å¼(ä¸æ˜¯è¢«åˆ¥äºº import ä½¿ç”¨æ™‚)ã€‚
    if os.path.exists("model.pth"):
        print("âœ… å·²æ‰¾åˆ° model.pth,è¼‰å…¥æ¨¡å‹è·³éè¨“ç·´")
        model.load_state_dict(torch.load("model.pth", map_location=device))
    else:
        print("ğŸš€ æœªæ‰¾åˆ° model.pth,é–‹å§‹è¨“ç·´æ¨¡å‹...")
        num_epochs = 15 # batch_sizeè¨­å®šåœ¨train_loaderå’Œtest_loaderé‚£
        for epoch in range(num_epochs):
            model.train() # å¾nn.Moduleç¹¼æ‰¿çš„æ–¹æ³•ã€‚å°‡æ¨¡å‹è¨­ç‚ºã€Œè¨“ç·´æ¨¡å¼ã€,æœ‰äº›å±¤(åƒ Dropout æˆ– BatchNorm)æœƒå•Ÿç”¨è¨“ç·´è¡Œç‚ºã€‚
            total_loss = 0.0
            for batch in train_loader:
            
                # æ¸…ç†èˆŠæ¢¯åº¦,ä»¥å…ç´¯åŠ ã€‚ç‚ºç”šéº¼è¦?å› ç‚ºPyTorch é è¨­æ¯æ¬¡å‘¼å« .backward() éƒ½æœƒã€Œç´¯åŠ ã€æ¢¯åº¦(ä¸æœƒè‡ªå‹•æ¸…æ‰)
                # æ²’ .zero_grad(),æ¢¯åº¦æœƒè¶Šç´¯ç©è¶Šå¤š,æ¨¡å‹æœƒäº‚æ‰ã€‚
                optimizer.zero_grad() 
                
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                token_type_ids = batch["token_type_ids"].to(device)
                labels = batch["labels"].to(device)
                outputs = model(input_ids, attention_mask, token_type_ids)
                
                loss = criterion(outputs, labels) # æ¯”è¼ƒ é æ¸¬çµæœ outputs(Sigmoid çš„æ©Ÿç‡)å’Œ çœŸå¯¦ç­”æ¡ˆ labels
                
                # ç”¨éˆå¼æ³•å‰‡(Chain Rule)è¨ˆç®—æ¯ä¸€å±¤ã€Œåƒæ•¸å° loss çš„å½±éŸ¿ã€,ä¹Ÿå°±æ˜¯æ¢¯åº¦
                # PyTorch åˆ©ç”¨è‡ªå‹•å¾®åˆ†(autograd)å¹«ä½ è¨ˆç®—æ•´å€‹è¨ˆç®—åœ–çš„åå°æ•¸,ç„¶å¾Œå­˜åœ¨æ¯ä¸€å±¤çš„ .grad è£¡ã€‚
                loss.backward()
                
                # ç”¨ .grad ä¸­çš„æ¢¯åº¦è³‡è¨Šæ ¹
                # æ“šå­¸ç¿’ç‡å’Œå„ªåŒ–å™¨çš„è¦å‰‡
                # æ”¹è®Šæ¯ä¸€å€‹åƒæ•¸çš„å€¼,ä»¥è®“ä¸‹ä¸€æ¬¡é æ¸¬æ›´æ¥è¿‘çœŸå¯¦
                optimizer.step()
                
                # loss æ˜¯ä¸€å€‹ tensor(éœ€è¦ backward);.item() æŠŠå®ƒè½‰æˆ Python çš„ç´”æ•¸å­—(float)
                total_loss += loss.item()
            print(f"[Epoch{epoch+1}]Training Loss:{total_loss:.4f}")
        torch.save(model.state_dict(), "model.pth")# å„²å­˜æ¨¡å‹æ¬Šé‡
        print("âœ… æ¨¡å‹è¨“ç·´å®Œæˆä¸¦å„²å­˜ç‚º model.pth")
"""

"""
æ•´å€‹æ¨¡å‹ä¸­æ¯ä¸€å€‹æ–‡å­—(token)å§‹çµ‚æ˜¯ä¸€å€‹å‘é‡,éš¨è‘—å±¤æ•¸ä¸åŒ,é€™å€‹å‘é‡ä»£è¡¨çš„æ„ç¾©æœƒæ›´é«˜éšã€æ›´èªæ„ã€æ›´æŠ½è±¡ã€‚
åœ¨æ•´å€‹ BERT + LSTM + CNN æ¨¡å‹çš„æµç¨‹ä¸­,ã€Œæ¯ä¸€å€‹æ–‡å­—(token)ã€éƒ½æœƒè¢«è¡¨ç¤ºæˆä¸€å€‹ã€Œå‘é‡ã€ä¾†é€²è¡Œå¾ŒçºŒçš„è¨ˆç®—èˆ‡å­¸ç¿’ã€‚
ä»Šå¤©æˆ‘è¼¸å…¥ä¸€å€‹å¥å­:"æ—©å®‰ä½ å¥½,åƒé£¯æ²’"
BERT çš„è¼¸å…¥åŒ…å«ä¸‰å€‹éƒ¨åˆ†:input_idsã€attention_maskã€token_type_ids,
é€™äº›æ˜¯ BERT æ‰€éœ€çš„æ ¼å¼ã€‚BERT æœƒå°‡å¥å­ä¸­æ¯å€‹ token ç·¨ç¢¼ç‚ºä¸€å€‹ 768 ç¶­çš„èªæ„å‘é‡,

é€²å…¥ BERT â†’ æ¯å€‹ token è®Šæˆèªæ„å‘é‡:
BERT è¼¸å‡ºæ¯å€‹å­—ç‚ºä¸€å€‹ 768 ç¶­çš„èªæ„å‘é‡
ã€Œæ—©ã€ â†’ [0.23, -0.11, ..., 0.45]   é•·åº¦ç‚º 768
ã€Œå®‰ã€ â†’ [0.05, 0.33, ..., -0.12]   ä¸€æ¨£ 768
...
batch size æ˜¯ 8,å¥å­é•·åº¦æ˜¯ 8,è¼¸å‡º shape ç‚º:    
[batch_size=8, seq_len=8, hidden_size=768]

æ¥ä¸‹ä¾†é€™äº›å‘é‡æœƒè¼¸å…¥åˆ° LSTM,LSTMä¸æœƒæ”¹è®Šã€Œä¸€å€‹tokenæ˜¯ä¸€å€‹å‘é‡ã€çš„æ¦‚å¿µ,è€Œæ˜¯é‡æ–°è¡¨ç¤ºæ¯å€‹tokençš„èªå¢ƒå‘é‡ã€‚
æŠŠæ¯å€‹åŸæœ¬ 768 ç¶­çš„ token å£“ç¸®æˆ hidden_size=128,é›™å‘ LSTM â†’ æ‹¼æ¥ â†’ æ¯å€‹ token æˆç‚º 256 ç¶­å‘é‡:

input_size=768 æ˜¯å¾ BERT æ¥æ”¶çš„å‘é‡ç¶­åº¦
hidden_size=128 è¡¨ç¤ºæ¯å€‹æ–¹å‘çš„ LSTM æœƒæŠŠ token å£“ç¸®ç‚º 128 ç¶­èªæ„å‘é‡
num_layers=1 è¡¨ç¤ºåªå †ç–Š 1 å±¤ LSTM
bidirectional=True è¡¨ç¤ºæ˜¯é›™å‘

LSTM,é™¤äº†å¾å·¦è®€åˆ°å³,ä¹Ÿæœƒå¾å³è®€åˆ°å·¦,å…©å€‹æ–¹å‘çš„è¼¸å‡ºæœƒåˆä½µ(æ‹¼æ¥),è®Šæˆ:
[batch_size=8, seq_len=8, hidden_size=256]  # å› ç‚º128*2

æ¥ä¸‹ä¾†é€²å…¥ CNN,CNN ä»ç„¶ä»¥ã€Œä¸€å€‹å‘é‡ä»£è¡¨ä¸€å€‹å­—ã€çš„å½¢å¼è™•ç†:

in_channels=256(å› ç‚º LSTM æ˜¯é›™å‘è¼¸å‡º)

out_channels=128 è¡¨ç¤ºå­¸ç¿’å‡º 128 å€‹æ¿¾æ³¢å™¨,æ¯å€‹æ¿¾æ³¢å™¨å°ˆé–€æŠ“ä¸€ç¨® n-gram(ä¾‹å¦‚ã€Œæ—©å®‰ä½ ã€),æ¯å€‹ã€Œç‰‡æ®µã€çš„çµæœè¼¸å‡ºç‚º 128 ç¶­ç‰¹å¾µ

kernel_size=3 è¡¨ç¤ºæ¯å€‹æ¿¾æ³¢å™¨çœ‹ 3 å€‹é€£çºŒ token(åƒæ˜¯ä¸€å€‹ 3-gram)æˆ–,æŠŠç›¸é„°çš„ 3 å€‹å­—(å„ç‚º 256 ç¶­)ä¸€èµ·æƒæ

padding=1 ç‚ºäº†ä¿ç•™è¼¸å‡ºåºåˆ—é•·åº¦å’Œè¼¸å…¥ç›¸åŒ,é¿å…é‚Šç•Œè³‡è¨Šè¢«æ¨æ£„

CNN è¼¸å‡ºçš„ shape å°±æœƒæ˜¯:

[batch_size=8, out_channels=128, seq_len=8],é‚„æ˜¯æ¯å€‹ token æœ‰å°æ‡‰ä¸€å€‹å‘é‡(åªæ˜¯é€™å‘é‡æ˜¯ CNN æŠ½å‡ºçš„æ–°ç‰¹å¾µ)

"""