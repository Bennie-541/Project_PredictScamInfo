"""
æµç¨‹åœ–
è®€å–è³‡æ–™ â†’ åˆ†å‰²è³‡æ–™ â†’ ç·¨ç¢¼ â†’ å»ºç«‹ Dataset / DataLoader
â†“
å»ºç«‹æ¨¡å‹(BERT+LSTM+CNN)
        â†“
        BERT è¼¸å‡º [batch, seq_len, 768]
        â†“
        BiLSTM  [batch, seq_len, hidden_dim*2]
        â†“
        CNN æ¨¡çµ„ (Conv1D + Dropout + GlobalMaxPooling1D)
        â†“
        Linear åˆ†é¡å™¨(è¼¸å‡ºè©é¨™æ©Ÿç‡)
        â†“
è¨“ç·´æ¨¡å‹(Epochs)
â†“
è©•ä¼°æ¨¡å‹(Accuracy / F1 / Precision / Recall)
â†“
å„²å­˜æ¨¡å‹(.pth)

"""
#å¼•å…¥é‡è¦å¥—ä»¶Import Library

import os
import torch                            #   PyTorch ä¸»æ¨¡çµ„               
import torch.nn as nn                   #	ç¥ç¶“ç¶²è·¯ç›¸é—œçš„å±¤(ä¾‹å¦‚ LSTMã€Linear)        
import pandas as pd
import re
import ast
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score, 
    precision_score, recall_score, f1_score, roc_auc_score,
    precision_recall_curve, auc, matthews_corrcoef
)
from dotenv import load_dotenv
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset #	æä¾› Datasetã€DataLoader é¡åˆ¥
from transformers import BertTokenizer # BertTokenizeræŠŠæ–‡å­—å¥å­è½‰æ›æˆ BERT æ ¼å¼çš„ token ID,ä¾‹å¦‚ [CLS] ä»Šå¤© å¤©æ°£ ä¸éŒ¯ [SEP] â†’ [101, 1234, 5678, ...]
from sklearn.model_selection import train_test_split

from transformers import BertModel
"""
# ------------------- è¼‰å…¥ .env ç’°å¢ƒè®Šæ•¸ -------------------

path = r"E:\Project_PredictScamInfo"

# ------------------- ä½¿ç”¨ç›¸å°è·¯å¾‘æ‰¾ CSV -------------------

#å¦‚æœ‰éœ€è¦è¨“ç·´è¤‡æ•¸ç­†è³‡æ–™å¯ä»¥ä½¿ç”¨é€™å€‹æ–¹æ³•csv_files = [os.path.join(base_dir, "æª”æ¡ˆåç¨±1.csv"),os.path.join(base_dir, "æª”æ¡ˆåç¨±2.csv")]
#ç¨‹å¼ç¢¼ä¸€è‡³131è¡Œ

# GPU è¨˜æ†¶é«”é™åˆ¶(å¯é¸)
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:16"

#è³‡æ–™å‰è™•ç†

class BertPreprocessor:
    def __init__(self, tokenizer_name="ckiplab/bert-base-chinese", max_len=128):
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)
        self.max_len = max_len

    def load_and_clean(self, filepath):
        df = pd.read_csv(filepath)
        df = df.dropna().drop_duplicates(subset=["message"]).reset_index(drop=True)

        # æ¸…ç† message æ¬„ä½
        df["message"] = df["message"].astype(str)
        df["message"] = df["message"].apply(lambda text: re.sub(r"\s+", "", text))
        df["message"] = df["message"].apply(lambda text: re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚,ï¼ï¼Ÿ]", "", text))

        # æ¸…ç† keywords æ¬„ä½ï¼ˆå¦‚æœæœ‰ï¼‰
        if "keywords" in df.columns:
            df["keywords"] = df["keywords"].fillna("")
            df["keywords"] = df["keywords"].apply(
                lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith("[") else []
            )
            df["keywords"] = df["keywords"].apply(
                lambda lst: [re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚,ï¼ï¼Ÿ]", "", str(k)) for k in lst]
            )
            df["keywords"] = df["keywords"].apply(lambda lst: "ã€‚".join(lst))
        else:
            df["keywords"] = ""

        # åˆä½µç‚º BERT è¼¸å…¥å…§å®¹
        df["combined"] = df["message"] + "ã€‚" + df["keywords"]
        return df[["combined", "label"]]

    def encode(self, texts):
        return self.tokenizer(
            list(texts),
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=self.max_len
        )
#è‡ªå‹•åšè³‡æ–™å‰è™•ç†
def build_bert_inputs(files):
    processor = BertPreprocessor()
    dfs = [processor.load_and_clean(f) for f in files]
    all_df = pd.concat(dfs, ignore_index=True)
    print("ğŸ“Œ åŸå§‹è³‡æ–™ç­†æ•¸ï¼š", sum(len(pd.read_csv(f)) for f in files))
    print("ğŸ“Œ æ¸…ç†å¾Œè³‡æ–™ç­†æ•¸ï¼š", len(all_df))
    print(f"âœ… å·²è®€å…¥ {len(all_df)} ç­†è³‡æ–™")
    print(all_df["label"].value_counts())
    print("ğŸ“Œ åˆä½µå¾Œè¼¸å…¥ç¤ºä¾‹ï¼š")
    print(all_df["combined"].head())

    train_df, val_df = train_test_split(
        all_df,
        stratify=all_df["label"],
        test_size=0.2,
        random_state=25,
        shuffle=True
    )

    train_inputs = processor.encode(train_df["combined"])
    val_inputs = processor.encode(val_df["combined"])

    return train_inputs, train_df["label"], val_inputs, val_df["label"], processor


#å®šç¾© PyTorch Dataset é¡åˆ¥ã€‚ScamDataset ç¹¼æ‰¿è‡ª torch.utils.data.Dataset
#å°‡ BERT è¼¸å‡ºçš„ token èˆ‡å°æ‡‰æ¨™ç±¤å°è£æˆ PyTorch èƒ½ä½¿ç”¨çš„æ ¼å¼
class ScamDataset(Dataset):
    def __init__(self, inputs, labels):
        self.input_ids = inputs["input_ids"]                           # input_ids:å¥å­çš„ token ID;attention_mask:æ³¨æ„åŠ›é®ç½©(0 = padding)
        self.attention_mask = inputs["attention_mask"]                 # token_type_ids:å¥å­çš„ segment å€åˆ†
        self.token_type_ids = inputs["token_type_ids"]                 # torch.tensor(x, dtype=...)å°‡è³‡æ–™(x)è½‰ç‚ºTensorçš„æ¨™æº–åšæ³•ã€‚
        self.labels = torch.tensor(labels.values, dtype=torch.float32) # xå¯ä»¥æ˜¯ listã€NumPy arrayã€pandas series...
# dtypefloat32:æµ®é»æ•¸(å¸¸ç”¨æ–¼ å›æ­¸ æˆ– BCELoss äºŒåˆ†é¡);long:æ•´æ•¸(å¸¸ç”¨æ–¼ å¤šåˆ†é¡ æ­é… CrossEntropyLoss)ã€‚labels.values â†’ è½‰ç‚º NumPy array
    
    def __len__(self):          # å‘Šè¨´ PyTorch é€™å€‹ Dataset æœ‰å¹¾ç­†è³‡æ–™
        return len(self.labels) # çµ¦ len(dataset) æˆ– for i in range(len(dataset)) ç”¨çš„
    
    def __getitem__(self, idx): #æ¯æ¬¡èª¿ç”¨ __getitem__() å›å‚³ä¸€ç­† {input_ids, attention_mask, token_type_ids, labels}
        return {                #DataLoader æ¯æ¬¡æœƒå‘¼å«é€™å€‹æ–¹æ³•å¤šæ¬¡ä¾†æŠ“ä¸€å€‹ batch çš„è³‡æ–™
            "input_ids":self.input_ids[idx],
            "attention_mask":self.attention_mask[idx],
            "token_type_ids":self.token_type_ids[idx],
            "labels":self.labels[idx]
        }

# é€™æ¨£å¯ä»¥åŒæ™‚è™•ç† scam å’Œ normal è³‡æ–™,ä¸ç”¨é‡è¤‡å¯«æ¸…ç†èˆ‡ token è™•ç†
if __name__ == "__main__":
    csv_files = [os.path.join(path, r"Filled_Keyword_MessageDeduplicated.csv")]
    train_inputs, train_labels, val_inputs, val_labels, processor = build_bert_inputs(csv_files)
    
    train_dataset = ScamDataset(train_inputs, train_labels)
    val_dataset = ScamDataset(val_inputs, val_labels)

    # batch_sizeæ¯æ¬¡é€é€²æ¨¡å‹çš„æ˜¯ 8 ç­†è³‡æ–™(è€Œä¸æ˜¯ä¸€ç­†ä¸€ç­†)
    # æ¯æ¬¡å¾ Dataset ä¸­æŠ“ä¸€æ‰¹(batch)è³‡æ–™å‡ºä¾†
    train_loader = DataLoader(train_dataset, batch_size=128)
    val_loader = DataLoader(val_dataset, batch_size=128)
"""
"""
class BertLSTM_CNN_Classifier(nn.Module)è¡¨ç¤º:ä½ å®šç¾©äº†ä¸€å€‹å­é¡åˆ¥,
ç¹¼æ‰¿è‡ª PyTorch çš„åŸºç¤æ¨¡å‹é¡åˆ¥ nn.Moduleã€‚

è‹¥ä½ åœ¨ __init__() è£¡æ²’æœ‰å‘¼å« super().__init__(),
é‚£éº¼çˆ¶é¡åˆ¥ nn.Module çš„åˆå§‹åŒ–é‚è¼¯(åŒ…å«é‡è¦åŠŸèƒ½)å°±ä¸æœƒè¢«åŸ·è¡Œ,
å°è‡´æ•´å€‹æ¨¡å‹é‹ä½œç•°å¸¸æˆ–éŒ¯èª¤ã€‚
"""

# nn.Moduleæ˜¯PyTorchæ‰€æœ‰ç¥ç¶“ç¶²è·¯æ¨¡å‹çš„åŸºç¤é¡åˆ¥,nn.Module æ˜¯ PyTorch æ‰€æœ‰ç¥ç¶“ç¶²è·¯æ¨¡å‹çš„åŸºç¤é¡åˆ¥
class BertLSTM_CNN_Classifier(nn.Module):
    
    def __init__(self, hidden_dim=128, num_layers=1, dropout=0.3):
        
        # super()æ˜¯Pythonæä¾›çš„ä¸€å€‹æ–¹æ³•,ç”¨ä¾†å‘¼å«ã€Œçˆ¶é¡åˆ¥çš„ç‰ˆæœ¬ã€çš„æ–¹æ³•ã€‚
        # å‘¼å«:super().__init__()è®“çˆ¶é¡åˆ¥(nn.Module)è£¡é¢é‚£äº›åŠŸèƒ½ã€å±¬æ€§éƒ½è¢«æ­£ç¢ºåˆå§‹åŒ–ã€‚
        # æ²’super().__init__(),é€™äº›éƒ½ä¸æœƒæ­£ç¢ºé‹ä½œ,æ¨¡å‹æœƒå£æ‰ã€‚
        # super() å°±æ˜¯ Python æä¾›çµ¦ã€Œå­é¡åˆ¥å‘¼å«çˆ¶é¡åˆ¥æ–¹æ³•ã€çš„æ–¹å¼
        super().__init__()
        
        # è¼‰å…¥ä¸­æ–‡é è¨“ç·´çš„ BERT æ¨¡å‹,è¼¸å…¥ç‚ºå¥å­token IDs,è¼¸å‡ºç‚ºæ¯å€‹ token çš„å‘é‡,å¤§å°ç‚º [batch, seq_len, 768]ã€‚
        self.bert = BertModel.from_pretrained("ckiplab/bert-base-chinese") # é€™æ˜¯å¼•å…¥hugging faceä¸­çš„tranceformat
        
        # æ¥æ”¶BERTçš„è¼¸å‡º(768 ç¶­å‘é‡),é€²è¡Œé›™å‘LSTM(BiLSTM)å»ºæ¨¡,è¼¸å‡ºç‚º [batch, seq_len, hidden_dim*2],ä¾‹å¦‚ [batch, seq_len, 256]
        """
        LSTM æ¥æ”¶æ¯å€‹tokençš„768ç¶­å‘é‡(ä¾†è‡ª BERT)ä½œç‚ºè¼¸å…¥,
        é€éæ¯å€‹æ–¹å‘çš„LSTMå£“ç¸®æˆ128ç¶­çš„èªæ„å‘é‡ã€‚
        ç”±æ–¼æ˜¯é›™å‘LSTM,æœƒåŒæ™‚å¾å·¦åˆ°å³(å‰å‘)å’Œå³åˆ°å·¦(å¾Œå‘)å„åšä¸€æ¬¡,
        æœ€å¾Œå°‡å…©å€‹æ–¹å‘çš„è¼¸å‡ºåˆä½µç‚º256ç¶­å‘é‡(128Ã—2)ã€‚
        æ¯æ¬¡è™•ç†ä¸€å€‹ batch(ä¾‹å¦‚ 8 å¥è©±),ä¸€æ¬¡èµ°å®Œæ•´å€‹æ™‚é–“åºåˆ—ã€‚
        """
        self.LSTM = nn.LSTM(input_size=768,        
                            hidden_size=hidden_dim,
                            num_layers=num_layers,
                            batch_first=True,
                            bidirectional=True)
        
         # CNN æ¨¡çµ„:æ¥åœ¨ LSTM å¾Œçš„è¼¸å‡ºä¸Šã€‚å°‡LSTMçš„è¼¸å‡ºè½‰æˆå·ç©å±¤æ ¼å¼,é©ç”¨æ–¼Conv1D,CNNå¯å­¸ç¿’ä½ç½®ä¸è®Šçš„å±€éƒ¨ç‰¹å¾µã€‚
        self.conv1 =  nn.Conv1d(in_channels=hidden_dim*2,
                                out_channels=128,
                                kernel_size=3,  # é€™è£¡kernel_size=3 ç‚º 3-gram ç‰¹å¾µ
                                padding=1)
        
        self.dropout = nn.Dropout(dropout) # éš¨æ©Ÿå°‡éƒ¨åˆ†ç¥ç¶“å…ƒè¨­ç‚º 0,ç”¨ä¾†é˜²æ­¢ overfittingã€‚
        
        self.global_maxpool = nn.AdaptiveAvgPool1d(1)  #å°‡ä¸€æ•´å¥è©±çš„ç‰¹å¾µæ¿ƒç¸®æˆä¸€å€‹å›ºå®šå¤§å°çš„å¥å­è¡¨ç¤ºå‘é‡
        
        # å°‡CNNè¼¸å‡ºçš„128ç¶­ç‰¹å¾µå‘é‡è¼¸å‡ºç‚ºä¸€å€‹ã€Œæ©Ÿç‡å€¼ã€(è©é¨™æˆ–éè©é¨™)ã€‚
        self.classifier = nn.Linear(128,1)
        
    def forward(self, input_ids, attention_mask, token_type_ids):
        #BERT ç·¨ç¢¼
        outputs = self.bert(input_ids=input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids)
        #.last_hidden_stateæ˜¯BertModel.from_pretrained(...)å…§éƒ¨çš„keyï¼Œæœƒè¼¸å‡º    [batch, seq_len, 768]
        hidden_states = outputs.last_hidden_state 

        # é€å…¥ BiLSTM
        # transpose(1, 2) çš„ç”¨é€”æ˜¯ï¼šè®“ LSTM è¼¸å‡ºçš„è³‡æ–™å½¢ç‹€ç¬¦åˆ CNN æ‰€è¦æ±‚çš„æ ¼å¼
        #   å‡è¨­ä½ åŸæœ¬ LSTM è¼¸å‡ºæ˜¯ï¼š    [batch_size, seq_len, hidden_dim*2] = [8, 128, 256]
        # ä½†CNN(Conv1d)çš„è¼¸å…¥æ ¼å¼éœ€è¦æ˜¯ï¼š[batch_size, in_channels, seq_len] = [8, 256, 128]
        # å› æ­¤ä½ éœ€è¦åšï¼š.transpose(1, 2)æŠŠ seq_len å’Œ hidden_dim*2 èª¿æ›
        LSTM_out, _ = self.LSTM(hidden_states)     # [batch, seq_len, hidden_dim*2]
        LSTM_out = LSTM_out.transpose(1, 2)        # [batch, hidden_dim*2, seq_len]

        # å·ç© + Dropout
        x = self.conv1(LSTM_out)                   # [batch, 128, seq_len]
        x = self.dropout(x)
        
        #å…¨å±€æ± åŒ–
        # .squeeze(dim) çš„ä½œç”¨æ˜¯ï¼šæŠŠæŸå€‹ã€Œç¶­åº¦å¤§å°ç‚º 1ã€çš„ç¶­åº¦åˆªæ‰
        # x = self.global_maxpool(x).squeeze(2)   # è¼¸å‡ºæ˜¯ [batch, 128, 1]
        # ä¸ .squeeze(2)ï¼Œä½ æœƒå¾—åˆ° shape ç‚º [batch, 128, 1]ï¼Œä¸æ–¹ä¾¿å¾Œé¢æ¥ Linearã€‚
        # .squeeze(2)=æ‹¿æ‰ç¬¬ 2 ç¶­ï¼ˆæ•¸å€¼æ˜¯ 1ï¼‰ â†’ è®“å½¢ç‹€è®Šæˆ [batch, 128]
        x = self.global_maxpool(x).squeeze(2)      # [batch, 128]

        #åˆ†é¡ & Sigmoid æ©Ÿç‡è¼¸å‡º
        logits = self.classifier(x)
        
        #.sigmoid() â†’ æŠŠ logits è½‰æˆ 0~1 çš„æ©Ÿç‡.squeeze() â†’ è®Šæˆä¸€ç¶­ [batch] é•·åº¦çš„æ©Ÿç‡ list
        """ä¾‹å¦‚ï¼š
logits = [[0.92], [0.05], [0.88], [0.41], ..., [0.17]]
â†’ sigmoid â†’ [[0.715], [0.512], ...]
â†’ squeeze â†’ [0.715, 0.512, ...]
"""
        return logits.squeeze() # æœ€å¾Œè¼¸å‡ºæ˜¯ä¸€å€‹å€¼ä»‹æ–¼ 0 ~ 1 ä¹‹é–“,ä»£è¡¨ã€Œç‚ºè©é¨™è¨Šæ¯çš„æ©Ÿç‡ã€ã€‚
"""
# è¨­å®š GPU è£ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åˆå§‹åŒ–æ¨¡å‹
model = BertLSTM_CNN_Classifier().to(device)
# å®šç¾© optimizer å’Œæå¤±å‡½æ•¸
optimizer = torch.optim.Adam(model.parameters(),lr=2e-5)
pos_weight = torch.tensor([2.13], dtype=torch.float32).to(device)
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

# æœ¬æ©Ÿè¨“ç·´è¿´åœˆ,è¦è¨“ç·´å†å–æ¶ˆè¨»è§£,å¦å‰‡åœ¨ç·šä¸Šç‰ˆæœ¬ä¸€å¾‹è™•æ–¼è¨»è§£ç‹€æ…‹
# è¨“ç·´æœŸé–“ç”¨çš„ç°¡åŒ–ç‰ˆé©—è­‰å‡½å¼ (åªå›å‚³ loss / acc)
def evaluate_epoch(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    all_labels, all_preds = [], []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            token_type_ids = batch["token_type_ids"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_ids, attention_mask, token_type_ids)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

            preds = (torch.sigmoid(outputs) > 0.5).long()
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())

    acc = accuracy_score(all_labels, all_preds)
    avg_loss = total_loss / len(dataloader)
    return avg_loss, acc

# ä¿®æ”¹è¨“ç·´ä¸»ç¨‹å¼

def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=15, save_path="model.pth"):
    train_loss_list, val_loss_list = [], []
    train_acc_list, val_acc_list = [], []

    for epoch in range(num_epochs):
        model.train()
        total_train_loss = 0
        train_true, train_pred = [], []

        for batch in train_loader:
            optimizer.zero_grad()
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            token_type_ids = batch["token_type_ids"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_ids, attention_mask, token_type_ids)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_train_loss += loss.item()
            preds = (torch.sigmoid(outputs) > 0.5).long()
            train_true.extend(labels.cpu().numpy())
            train_pred.extend(preds.cpu().numpy())

        train_acc = accuracy_score(train_true, train_pred)
        train_loss = total_train_loss / len(train_loader)

        val_loss, val_acc = evaluate_epoch(model, val_loader, criterion, device)

        train_loss_list.append(train_loss)
        val_loss_list.append(val_loss)
        train_acc_list.append(train_acc)
        val_acc_list.append(val_acc)

        print(f"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}")

    torch.save(model.state_dict(), save_path)
    print(f"âœ… æ¨¡å‹è¨“ç·´å®Œæˆä¸¦å„²å­˜ç‚º {save_path}")

    # å¯è¦–åŒ– Loss Curve
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, num_epochs+1), train_loss_list, label="Train Loss")
    plt.plot(range(1, num_epochs+1), val_loss_list, label="Val Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Loss Curve")
    plt.legend()
    plt.show()

    # å¯è¦–åŒ– Accuracy Curve
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, num_epochs+1), train_acc_list, label="Train Accuracy")
    plt.plot(range(1, num_epochs+1), val_acc_list, label="Val Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Accuracy Curve")
    plt.legend()
    plt.show()

        # è¨“ç·´çµæŸå¾Œç¹ªè£½ Loss Curve
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, num_epochs+1), train_loss_list, label="Train Loss")
    plt.plot(range(1, num_epochs+1), val_loss_list, label="Val Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Loss Curve")
    plt.legend()
    plt.show()

        # ç¹ªè£½ Accuracy Curve
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, num_epochs+1), train_acc_list, label="Train Accuracy")
    plt.plot(range(1, num_epochs+1), val_acc_list, label="Val Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Accuracy Curve")
    plt.legend()
    plt.show()

def evaluate_model(model, val_loader, device):
    model.eval()
    y_true = []
    y_pred = []
    y_pred_prob = []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            token_type_ids = batch["token_type_ids"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_ids, attention_mask, token_type_ids)
            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).long()

            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())
            y_pred_prob.extend(probs.cpu().numpy())

    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    spec = recall_score(y_true, y_pred, pos_label=0)
    mcc = matthews_corrcoef(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_pred_prob)
    precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_pred_prob)
    pr_auc = auc(recall_curve, precision_curve)

    metrics_dict = {
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'Specificity': spec,
        'F1-score': f1,
        'MCC': mcc,
        'ROC AUC': roc_auc,
        'PR AUC': pr_auc
    }

    # è¦–è¦ºåŒ–ï¼šæ•´é«”æŒ‡æ¨™ bar chart
    metric_names = list(metrics_dict.keys())
    metric_values = list(metrics_dict.values())

    plt.figure(figsize=(10, 6))
    sns.barplot(y=metric_names, x=metric_values, palette="Blues_d")
    for index, value in enumerate(metric_values):
        plt.text(value + 0.01, index, f"{value:.4f}", va='center')
    plt.title("æ¨¡å‹è©•ä¼°æŒ‡æ¨™")
    plt.xlim(0, 1.05)
    plt.xlabel("Score")
    plt.ylabel("Metric")
    plt.grid(axis='x', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

    # è¦–è¦ºåŒ–ï¼šConfusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Scam (0)", "Normal (1)"], yticklabels=["Scam (0)", "Normal (1)"])
    plt.xlabel("Predict")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.tight_layout()
    plt.show()

    # PR Curve (é¡å¤– bonus)
    plt.plot(recall_curve, precision_curve, marker='.')
    plt.title('Precision-Recall Curve')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.show()

    # ROC Curve (é¡å¤– bonus)
    from sklearn.metrics import RocCurveDisplay
    RocCurveDisplay.from_predictions(y_true, y_pred_prob)
    plt.title('ROC Curve')
    plt.show()


# æ”¾åœ¨ä¸»ç¨‹å¼ä¸­å‘¼å«
if __name__ == "__main__":
    print("âœ… é–‹å§‹é©—è­‰æ¨¡å‹æ•ˆæœ...")
    evaluate_model(model, val_loader, device)
"""
"""

æ•´å€‹æ¨¡å‹ä¸­æ¯ä¸€å€‹æ–‡å­—(token)å§‹çµ‚æ˜¯ä¸€å€‹å‘é‡,éš¨è‘—å±¤æ•¸ä¸åŒ,é€™å€‹å‘é‡ä»£è¡¨çš„æ„ç¾©æœƒæ›´é«˜éšã€æ›´èªæ„ã€æ›´æŠ½è±¡ã€‚
åœ¨æ•´å€‹ BERT + LSTM + CNN æ¨¡å‹çš„æµç¨‹ä¸­,ã€Œæ¯ä¸€å€‹æ–‡å­—(token)ã€éƒ½æœƒè¢«è¡¨ç¤ºæˆä¸€å€‹ã€Œå‘é‡ã€ä¾†é€²è¡Œå¾ŒçºŒçš„è¨ˆç®—èˆ‡å­¸ç¿’ã€‚
ä»Šå¤©æˆ‘è¼¸å…¥ä¸€å€‹å¥å­:"æ—©å®‰ä½ å¥½,åƒé£¯æ²’"
BERT çš„è¼¸å…¥åŒ…å«ä¸‰å€‹éƒ¨åˆ†:input_idsã€attention_maskã€token_type_ids,
é€™äº›æ˜¯ BERT æ‰€éœ€çš„æ ¼å¼ã€‚BERT æœƒå°‡å¥å­ä¸­æ¯å€‹ token ç·¨ç¢¼ç‚ºä¸€å€‹ 768 ç¶­çš„èªæ„å‘é‡,

é€²å…¥ BERT â†’ æ¯å€‹ token è®Šæˆèªæ„å‘é‡:
BERT è¼¸å‡ºæ¯å€‹å­—ç‚ºä¸€å€‹ 768 ç¶­çš„èªæ„å‘é‡
ã€Œæ—©ã€ â†’ [0.23, -0.11, ..., 0.45]   é•·åº¦ç‚º 768
ã€Œå®‰ã€ â†’ [0.05, 0.33, ..., -0.12]   ä¸€æ¨£ 768
...
batch size æ˜¯ 8,å¥å­é•·åº¦æ˜¯ 8,è¼¸å‡º shape ç‚º:    
[batch_size=8, seq_len=8, hidden_size=768]

æ¥ä¸‹ä¾†é€™äº›å‘é‡æœƒè¼¸å…¥åˆ° LSTM,LSTMä¸æœƒæ”¹è®Šã€Œä¸€å€‹tokenæ˜¯ä¸€å€‹å‘é‡ã€çš„æ¦‚å¿µ,è€Œæ˜¯é‡æ–°è¡¨ç¤ºæ¯å€‹tokençš„èªå¢ƒå‘é‡ã€‚
æŠŠæ¯å€‹åŸæœ¬ 768 ç¶­çš„ token å£“ç¸®æˆ hidden_size=128,é›™å‘ LSTM â†’ æ‹¼æ¥ â†’ æ¯å€‹ token æˆç‚º 256 ç¶­å‘é‡:

input_size=768 æ˜¯å¾ BERT æ¥æ”¶çš„å‘é‡ç¶­åº¦
hidden_size=128 è¡¨ç¤ºæ¯å€‹æ–¹å‘çš„ LSTM æœƒæŠŠ token å£“ç¸®ç‚º 128 ç¶­èªæ„å‘é‡
num_layers=1 è¡¨ç¤ºåªå †ç–Š 1 å±¤ LSTM
bidirectional=True è¡¨ç¤ºæ˜¯é›™å‘

LSTM,é™¤äº†å¾å·¦è®€åˆ°å³,ä¹Ÿæœƒå¾å³è®€åˆ°å·¦,å…©å€‹æ–¹å‘çš„è¼¸å‡ºæœƒåˆä½µ(æ‹¼æ¥),è®Šæˆ:
[batch_size=8, seq_len=8, hidden_size=256]  # å› ç‚º128*2

æ¥ä¸‹ä¾†é€²å…¥ CNN,CNN ä»ç„¶ä»¥ã€Œä¸€å€‹å‘é‡ä»£è¡¨ä¸€å€‹å­—ã€çš„å½¢å¼è™•ç†:

in_channels=256(å› ç‚º LSTM æ˜¯é›™å‘è¼¸å‡º)

out_channels=128 è¡¨ç¤ºå­¸ç¿’å‡º 128 å€‹æ¿¾æ³¢å™¨,æ¯å€‹æ¿¾æ³¢å™¨å°ˆé–€æŠ“ä¸€ç¨® n-gram(ä¾‹å¦‚ã€Œæ—©å®‰ä½ ã€),æ¯å€‹ã€Œç‰‡æ®µã€çš„çµæœè¼¸å‡ºç‚º 128 ç¶­ç‰¹å¾µ

kernel_size=3 è¡¨ç¤ºæ¯å€‹æ¿¾æ³¢å™¨çœ‹ 3 å€‹é€£çºŒ token(åƒæ˜¯ä¸€å€‹ 3-gram)æˆ–,æŠŠç›¸é„°çš„ 3 å€‹å­—(å„ç‚º 256 ç¶­)ä¸€èµ·æƒæ

padding=1 ç‚ºäº†ä¿ç•™è¼¸å‡ºåºåˆ—é•·åº¦å’Œè¼¸å…¥ç›¸åŒ,é¿å…é‚Šç•Œè³‡è¨Šè¢«æ¨æ£„

CNN è¼¸å‡ºçš„ shape å°±æœƒæ˜¯:

[batch_size=8, out_channels=128, seq_len=8],é‚„æ˜¯æ¯å€‹ token æœ‰å°æ‡‰ä¸€å€‹å‘é‡(åªæ˜¯é€™å‘é‡æ˜¯ CNN æŠ½å‡ºçš„æ–°ç‰¹å¾µ)

"""