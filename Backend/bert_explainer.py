import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import shap
import torch                
import re
import easyocr
import io
import numpy as np
from PIL import Image
from huggingface_hub import hf_hub_download
from transformers import BertTokenizer
from AI_Model_architecture import BertLSTM_CNN_Classifier

# OCR æ¨¡çµ„
reader = easyocr.Reader(['ch_tra', 'en'], gpu=torch.cuda.is_available())

# è¨­å®šè£ç½®ï¼ˆGPU å„ªå…ˆï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer

def load_model_and_tokenizer():
    global model, tokenizer

    if os.path.exists("model.pth"):
        model_path = "model.pth"
    else:
        model_path = hf_hub_download(repo_id="Bennie12/Bert-Lstm-Cnn-ScamDetecter", filename="model.pth")

    model = BertLSTM_CNN_Classifier()
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()

    tokenizer = BertTokenizer.from_pretrained("ckiplab/bert-base-chinese", use_fast=False)

    return model, tokenizer

model, tokenizer = load_model_and_tokenizer()
model.eval()

# é æ¸¬å–®ä¸€å¥å­çš„åˆ†é¡çµæœ
def predict_single_sentence(model, tokenizer, sentence, max_len=256):
    sentence = re.sub(r"\s+", "", sentence)
    sentence = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚ï¼Œï¼ï¼Ÿ:/._-]", "", sentence)

    encoded = tokenizer(sentence, return_tensors="pt", truncation=True, padding="max_length", max_length=max_len)
    encoded = {k: v.to(device) for k, v in encoded.items()}

    with torch.no_grad():
        output = model(encoded["input_ids"], encoded["attention_mask"], encoded["token_type_ids"])
        prob = output.item()
        label = int(prob > 0.5)

    risk = "ğŸŸ¢ ä½é¢¨éšªï¼ˆæ­£å¸¸ï¼‰"
    if prob > 0.9:
        risk = "ğŸ”´ é«˜é¢¨éšªï¼ˆæ¥µå¯èƒ½æ˜¯è©é¨™ï¼‰"
    elif prob > 0.5:
        risk = "ğŸŸ¡ ä¸­é¢¨éšªï¼ˆå¯ç–‘ï¼‰"

    pre_label = 'è©é¨™' if label == 1 else 'æ­£å¸¸'
    
    return {
        "label": pre_label,
        "prob": prob,
        "risk": risk
    }

# åŒ…è£æ¨¡å‹ä¾› SHAP ä½¿ç”¨
class ModelWrapper:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def __call__(self, texts):
        if isinstance(texts, np.ndarray):
            texts = texts.tolist()
        elif isinstance(texts, str):
            texts = [texts]

        encoded = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=256)
        encoded = {k: v.to(device) for k, v in encoded.items()}

        with torch.no_grad():
            outputs = self.model(encoded["input_ids"], encoded["attention_mask"], encoded["token_type_ids"])
            if len(outputs.shape) == 0:
                outputs = outputs.unsqueeze(0)
            probs = torch.stack([1-outputs, outputs], dim=1)
        return probs.cpu().numpy()

# å»ºç«‹èƒŒæ™¯è³‡æ–™
background_data = [
    "ä»Šå¤©ä¸‹ç­å¾Œæœ‰ç©ºå—ï¼Ÿæƒ³ç´„ä½ å»é€›é€›æ–°é–‹çš„å¤œå¸‚ï¼Œè½èªªå¾ˆå¥½åƒã€‚",
    "ä½ çŸ¥é“æ€éº¼ç”¨åœ–å½¢è¨ˆç®—é«”ç©å—ï¼Ÿæˆ‘æ‰¾äº†å¹¾å€‹ä¾‹é¡Œæˆ‘å€‘ä¸€èµ·ç·´ã€‚",
    "æ€¡è±ï¼Œè¨­è¨ˆéƒ¨äº¤ä¾†çš„ææ¡ˆä¸å¤ªç¬¦åˆè¦æ ¼ï¼Œä½ å¹«å¿™è¯çµ¡ä¸€ä¸‹èª¿æ•´å­—å‹èˆ‡é…è‰²è¦ç¯„ã€‚",
    "æ­å–œæ‚¨ï¼åœ¨æˆ‘å€‘çš„å¹´ç”¨æˆ¶å›é¥‹æ´»å‹•ä¸­ï¼Œæ‚¨æŠ½ä¸­ä¸€å°ï¼è«‹åœ¨å°æ™‚å…§å®Œæˆç™»è¨˜æ‰‹çºŒé ˜å–è£œç¹³é€¾æœŸå°‡æ”¶å–æ‰‹çºŒè²»ã€‚",
    "æ‚¨ä¿¡ç”¨å¡å­˜åœ¨é«˜é£é™©æ¶ˆè´¹è®°å½•ï¼Œéœ€æ”¯ä»˜å®‰å…¨ä¿éšœè´¹å…ƒï¼Œè¯¦æƒ…è¯·è®¿é—®",
    "æ‚¨å¥½æˆ‘å€‘æ˜¯å®˜æ–¹å®¢æœï¼Œç™¼ç¾æ‚¨çš„å¸³æˆ¶æœ‰ç•°å¸¸è¡Œç‚ºï¼Œè«‹å›è¦†æ‚¨çš„å¯†ç¢¼åŠæ‰‹æ©Ÿè™Ÿç¢¼ä»¥ä¾¿å”åŠ©è™•ç†ã€‚",
    "æ‚¨å¥½ï¼Œä¿¡ç”¨å¡è´¦å•å¼‚å¸¸è¯·å°½å¿«æ”¯ä»˜æœªç»“æ¬¾é¡¹å…ƒï¼Œå®¢æœçƒ­çº¿",
    "æ‚¨å¥½ï¼Œæ‚¨çš„æ‰‹æœºå¥—é¤å¼‚å¸¸ï¼Œè¯·æ”¯ä»˜è¡¥ç¼´è´¹ç”¨å…ƒï¼Œå¾®ä¿¡å®¢æœã€‚",
    "æ‚¨çš„æ‰‹æ©Ÿå·²ä¸­æ¯’ï¼Œæ‰€æœ‰æ‡‰ç”¨ç¨‹å¼æœ‰è¢«é ç«¯æ“æ§çš„å±éšªï¼Œè«‹é¦¬ä¸Šæƒæä¸¦åˆªé™¤å¯ç–‘è»Ÿé«”ï¼Œé»æ­¤ä¸‹è¼‰ã€‚",
    "æƒ³èµ·ä½ ç¬¬ä¸€æ¬¡è‡ªå·±æ­é£›æ©Ÿé‚£å¤©ï¼Œåª½åª½å…¶å¯¦è¶…ç·Šå¼µçš„ï¼Œé‚„å·å“­äº†ä¸€ä¸‹ã€‚",
    "æˆ‘ä¸æ˜¯ä½ æ“æ§çš„æ£‹å­ã€‚",
    "æˆ‘ç¾åœ¨çš„ç‹€æ…‹å°±æ˜¯ä¸€å€‹æœƒå‘¼å¸çš„å¤±èª¤",
    "æ“ å…¬è»Šçš„æ™‚å€™è¢«å¤¾åˆ°èƒŒåŒ…ï¼Œå¾Œé¢å¤§å¬¸é‚„ä¸€ç›´å‚¬æˆ‘ä¸‹è»Š",
    "æ”¿åºœå…¬å‘Šæ‚¨ç¬¦åˆè€äººç¦åˆ©æ´¥è²¼è³‡æ ¼ï¼Œè«‹é»æ“Š",
    "æœ€è¿‘æœ‰ä»€éº¼å¥½è½çš„éŸ³æ¨‚æ¨è–¦å—ï¼Ÿæˆ‘æƒ³æ›æ›å£å‘³ã€‚",
    "æœ‰äººèªªå°åŒ—ä¿¡ç¾©å•†åœˆå¤œæ™šæœ‰äººç©¿å¤è£é€›è¡—ï¼Œåƒç©¿è¶Šæ™‚ç©ºã€‚",
    "æœ¬è¡Œç³»çµ±å‡ç´šéœ€é‡æ–°é©—è­‰ç”¨æˆ¶è³‡æ–™ï¼Œè«‹æ–¼å°æ™‚å…§å®Œæˆèº«ä»½ç¢ºèªç¨‹åºï¼Œå¦å‰‡å°‡æš«æ™‚å‡çµå¸³æˆ¶æ‰€æœ‰äº¤æ˜“åŠŸèƒ½ã€‚",
    "æ°´åˆ©ç½²é€šçŸ¥ï¼Œæœˆåˆå°‡é€²è¡Œè‡ªä¾†æ°´ç®¡ç·šä¾‹è¡Œç¶­ä¿®ï¼Œå¯èƒ½çŸ­æš«åœæ°´ã€‚",
    "ç·Šæ€¥é€šçŸ¥æ‚¨æœ‰ä¸€ç­†é˜²ç–«è£œåŠ©æ¬¾å¾…é ˜å–ï¼Œè«‹é»æ“Šé€£çµä¸Šå‚³èº«ä»½è­‰ç…§ç‰‡åŠæ‰‹æ©Ÿé©—è­‰ç¢¼ä»¥å®Œæˆç”³è«‹ã€‚ç¶²å€",
    "è¦ªæ„›çš„ç”¨æˆ¶ï¼Œæ‚¨çš„é›»ä¿¡åˆç´„å³å°‡åˆ°æœŸï¼Œè«‹é»æ“Šé€£çµç¢ºèªçºŒç´„ä¸¦äº«å—å„ªæƒ æ–¹æ¡ˆï¼Œé€¾æœŸå°‡è‡ªå‹•çµ‚æ­¢æœå‹™ã€‚"
]

wrapped_model = ModelWrapper(model, tokenizer)
explainer = shap.Explainer(wrapped_model, tokenizer)

def clean_text(text):
    text = re.sub(r"https?://\S+", "", text)
    text = re.sub(r"[a-zA-Z0-9:/.%\-_=+]{4,}", "", text)
    text = re.sub(r"\+?\d[\d\s\-]{5,}", "", text)
    text = re.sub(r"[^ä¸€-é¾¥ã€‚ï¼Œï¼ï¼Ÿã€]", "", text)
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", text)
    cleaned = "ã€‚".join(sentences[:4])
    return cleaned[:300]

def suspicious_tokens(text, explainer, top_k=5):
    try:
        shap_values = explainer([text])
        tokens = shap_values.data[0]
        scores = shap_values.values[0]

        token_score = list(zip(tokens, scores))
        token_score.sort(key=lambda x: abs(x[1]), reverse=True)
        keywords = [t for t, s in token_score if len(t.strip()) > 1][:top_k]
        return keywords
    except Exception as e:
        print("âš  SHAP å¤±æ•—ï¼Œå•Ÿç”¨ fallback:", e)
        fallback = ["ç¹³è²»", "çµ‚æ­¢", "é€¾æœŸ", "é™æ™‚", "é©—è­‰ç¢¼"]
        return [kw for kw in fallback if kw in text]

def highlight_keywords(text, keywords):
    for word in keywords:
        text = text.replace(word, f"<span class='highlight'>{word}</span>")
    return text

def analyze_text(text):
    cleaned_text = clean_text(text)
    result = predict_single_sentence(model, tokenizer, cleaned_text)
    label = result["label"]
    prob = result["prob"]
    risk = result["risk"]

    suspicious = suspicious_tokens(cleaned_text, explainer)
    highlighted_text = highlight_keywords(text, suspicious)

    print(f"\nğŸ“© è¨Šæ¯å…§å®¹ï¼š{text}")
    print(f"âœ… é æ¸¬çµæœï¼š{label}")  
    print(f"ğŸ“Š ä¿¡å¿ƒå€¼ï¼š{round(prob*100, 2)}")
    print(f"âš ï¸ é¢¨éšªç­‰ç´šï¼š{risk}")
    print(f"å¯ç–‘é—œéµå­—æ“·å–: { [str(s).strip() for s in suspicious if isinstance(s, str) and len(s.strip()) > 1]}")

    return {
        "status": label,
        "confidence": round(prob * 100, 2),
        "suspicious_keywords":  suspicious,
        "highlighted_text": highlighted_text 
    }

def analyze_image(file_bytes):
    image = Image.open(io.BytesIO(file_bytes))
    image_np = np.array(image)
    results = reader.readtext(image_np)
    
    text = ' '.join([res[1] for res in results]).strip()
    
    if not text:
        return{
            "status" : "ç„¡æ³•è¾¨è­˜æ–‡å­—",
            "confidence" : 0.0,
            "suspicious_keywords" : ["åœ–ç‰‡ä¸­ç„¡å¯è¾¨è­˜çš„ä¸­æ–‡è‹±æ–‡"],
            "highlighted_text": "ç„¡æ³•è¾¨è­˜å¯ç–‘å…§å®¹"
        }
    return analyze_text(text)
