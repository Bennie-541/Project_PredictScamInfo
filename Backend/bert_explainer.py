
# å¼•å…¥é‡è¦å¥—ä»¶Import Library
# PyTorch ä¸»æ¨¡çµ„ï¼Œå’ŒTensorflowå¾ˆåƒ 
# å…±é€šé»ï¼šéƒ½æ˜¯æ·±åº¦å­¸ç¿’æ¡†æ¶ï¼Œæ”¯æ´å»ºæ§‹ç¥ç¶“ç¶²è·¯ã€è¨“ç·´èˆ‡æ¨è«–ï¼Œéƒ½æ”¯æ´GPUåŠ é€Ÿã€è¼‰å…¥æ¨¡å‹ï¼Œå’Œè™•ç†tensorç­‰ã€‚
# æ“ä½œæ¯”è¼ƒç›´è¦ºï¼Œæ¥è¿‘Pythonæœ¬èº«çš„é¢¨æ ¼ï¼Œå‹•æ…‹åœ–æ¶æ§‹(æ¯ä¸€æ¬¡forwardéƒ½å³æ™‚è¨ˆç®—)ï¼Œæ›´å®¹æ˜“é™¤éŒ¯ã€å¿«é€Ÿè¿­ä»£ï¼Œåœ¨ç ”ç©¶é ˜åŸŸéå¸¸æµè¡Œã€‚
# reæ˜¯Pythonå…§å»ºçš„æ­£å‰‡è¡¨ç¤ºå¼(regular expression)æ¨¡çµ„ï¼Œåœ¨é€™å°ˆæ¡ˆä¸­ç”¨ä¾†"ç”¨é—œéµè¦å‰‡ç¯©é¸æ–‡å­—å…§å®¹"ã€‚
# requestsæ˜¯ä¸€å€‹éå¸¸å¥½ç”¨çš„ HTTP è«‹æ±‚å¥—ä»¶ï¼Œèƒ½è®“ä½ å¾Pythonç™¼é€GET/POSTè«‹æ±‚ï¼Œåœ¨å°ˆæ¡ˆä¸­ç”¨ä¾†å¾Google Driveä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ(model.pth)ã€‚
# BertTokenizer:å¾Hugging Faceçš„transformerså¥—ä»¶è¼‰å…¥ä¸€å€‹å°ˆç”¨çš„ã€Œåˆ†è©å™¨ï¼ˆTokenizerï¼‰ã€ã€‚
# pip install shap

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import shap
import torch                
import re
import easyocr
import io
import numpy as np

from PIL import Image
from huggingface_hub import hf_hub_download
from transformers import BertTokenizer, pipeline
from AI_Model_architecture import BertLSTM_CNN_Classifier


reader = easyocr.Reader(['ch_tra', 'en'], gpu=torch.cuda.is_available())
# è¨­å®šè£ç½®ï¼ˆGPU å„ªå…ˆï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# é è¨­æ¨¡å‹èˆ‡ tokenizer ç‚º Noneï¼Œç›´åˆ°é¦–æ¬¡è«‹æ±‚æ‰è¼‰å…¥ï¼ˆå»¶é²è¼‰å…¥ï¼‰
# å…ˆè¼‰å…¥æ¨¡å‹èˆ‡ tokenizer


# âœ… å»¶é²è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer
def load_model_and_tokenizer():
    global model, tokenizer

    if os.path.exists("model.pth"):
        model_path = "model.pth"
    else:
        model_path = hf_hub_download(repo_id="Bennie12/Bert-Lstm-Cnn-ScamDetecter", filename="model.pth")

    from AI_Model_architecture import BertLSTM_CNN_Classifier
    model = BertLSTM_CNN_Classifier()
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()

    tokenizer = BertTokenizer.from_pretrained("ckiplab/bert-base-chinese", use_fast=False)

    return model, tokenizer

# é æ¸¬å–®ä¸€å¥å­çš„åˆ†é¡çµæœï¼ˆè©é¨™ or æ­£å¸¸ï¼‰
# model: è¨“ç·´å¥½çš„PyTorchæ¨¡å‹
# tokenizer: åˆ†è©å™¨ï¼Œè² è²¬æŠŠä¸­æ–‡è½‰æˆ BERT èƒ½è™•ç†çš„æ•¸å€¼æ ¼å¼
# sentence: ä½¿ç”¨è€…è¼¸å…¥çš„æ–‡å­—å¥å­
# max_len: é™åˆ¶æœ€å¤§è¼¸å…¥é•·åº¦ï¼ˆé è¨­ 256 å€‹ tokenï¼‰
def predict_single_sentence(model, tokenizer, sentence, max_len=256):
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ä½¿ç”¨ with torch.no_grad()ï¼Œä»£è¡¨é€™æ®µç¨‹å¼ã€Œä¸éœ€è¦è¨˜éŒ„æ¢¯åº¦ã€
    # é€™æ¨£å¯ä»¥åŠ é€Ÿæ¨è«–ä¸¦ç¯€çœè¨˜æ†¶é«”
    with torch.no_grad():
         # ----------- æ–‡å­—å‰è™•ç†ï¼šæ¸…æ´—è¼¸å…¥å¥å­ -----------
        sentence = re.sub(r"\s+", "", sentence)  # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—å…ƒï¼ˆç©ºæ ¼ã€æ›è¡Œç­‰ï¼‰
        sentence = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚ï¼Œï¼ï¼Ÿ:/._-]", "", sentence)
        # ä¿ç•™å¸¸è¦‹ä¸­æ–‡å­—ã€è‹±æ•¸å­—èˆ‡æ¨™é»ç¬¦è™Ÿï¼Œå…¶ä»–å¥‡æ€ªç¬¦è™Ÿéƒ½ç§»é™¤
        # ----------- ä½¿ç”¨ BERT Tokenizer å°‡å¥å­ç·¨ç¢¼ -----------
        encoded = tokenizer(sentence,
                            return_tensors="pt",       # å›å‚³ PyTorch tensor æ ¼å¼ï¼ˆé è¨­æ˜¯ numpy æˆ– listï¼‰
                            truncation=True,           # è¶…éæœ€å¤§é•·åº¦å°±æˆªæ–·
                            padding="max_length",      # ä¸è¶³æœ€å¤§é•·åº¦å‰‡è£œç©ºç™½ï¼ˆPAD tokenï¼‰
                            max_length=max_len)        # è¨­å®šæœ€å¤§é•·åº¦ç‚º 256
        # æŠŠ tokenizer å›å‚³çš„è³‡æ–™é€é€²æ¨¡å‹å‰ï¼Œto(device)è½‰åˆ°æŒ‡å®šçš„è£ç½®ï¼ˆGPU or CPUï¼‰
        input_ids = encoded["input_ids"].to(device)
        attention_mask = encoded["attention_mask"].to(device)
        token_type_ids = encoded["token_type_ids"].to(device)
        # ----------- æ¨¡å‹æ¨è«–ï¼šè¼¸å‡ºè©é¨™çš„æ©Ÿç‡å€¼ -----------
        output = model(input_ids, attention_mask, token_type_ids)# å›å‚³çš„æ˜¯ä¸€å€‹æ©Ÿç‡å€¼ï¼ˆfloatï¼‰
        prob = output.item()  # å¾ tensor å–å‡ºç´”æ•¸å­—ï¼Œä¾‹å¦‚ 0.86
        label = int(prob > 0.5)  # å¦‚æœæ©Ÿç‡ > 0.5ï¼Œæ¨™ç‚ºã€Œè©é¨™ã€ï¼ˆ1ï¼‰ï¼Œå¦å‰‡ç‚ºã€Œæ­£å¸¸ã€ï¼ˆ0ï¼‰
        # ----------- æ ¹æ“šæ©Ÿç‡é€²è¡Œé¢¨éšªåˆ†ç´š -----------
        if prob > 0.9:
            risk = "ğŸ”´ é«˜é¢¨éšªï¼ˆæ¥µå¯èƒ½æ˜¯è©é¨™ï¼‰"
        elif prob > 0.5:
            risk = "ğŸŸ¡ ä¸­é¢¨éšªï¼ˆå¯ç–‘ï¼‰"
        else:
            risk = "ğŸŸ¢ ä½é¢¨éšªï¼ˆæ­£å¸¸ï¼‰"
        # ----------- æ ¹æ“š label åˆ¤æ–·æ–‡å­—çµæœ -----------
        pre_label ='è©é¨™'if label == 1 else 'æ­£å¸¸'
        
        # çµ„æˆä¸€å€‹ Python å­—å…¸ï¼ˆå°æ‡‰ API çš„ JSON è¼¸å‡ºæ ¼å¼ï¼‰
        return {
        "label" : pre_label,                  # é æ¸¬åˆ†é¡ï¼ˆ"è©é¨™" or "æ­£å¸¸"ï¼‰
        "prob" : prob, # é æ¸¬åˆ†é¡ï¼ˆ"è©é¨™" or "æ­£å¸¸"ï¼‰  
        "risk" : risk     # ç”¨é¢¨éšªåˆ†ç´šç•¶ä½œ"å¯ç–‘æç¤º"æ”¾é€² listï¼ˆåç¨±ç‚º suspicious_keywordsï¼‰
    }

# analyze_text(text)å°æ‡‰app.pyç¬¬117è¡Œ
# é€™å€‹å‡½å¼æ˜¯ã€Œå°å¤–çš„ç°¡åŒ–ç‰ˆæœ¬ã€ï¼šè¼¸å…¥ä¸€å¥æ–‡å­— â†’ å›å‚³è©é¨™åˆ¤å®šçµæœ
# ç”¨åœ¨ä¸»ç¨‹å¼æˆ– FastAPI å¾Œç«¯ä¸­ï¼Œæ˜¯æ•´å€‹æ¨¡å‹é æ¸¬æµç¨‹çš„å…¥å£é»

# ----------- å¯ç–‘è©å¥æ“·å– -----------
class ModelWrapper:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def __call__(self, texts):
        if isinstance(texts, np.ndarray):
            texts = texts.tolist()
        elif isinstance(texts, str):
            texts = [texts]
        # å…¶å¯¦åˆ°é€™é‚Š texts å·²ç¶“æ˜¯ list of str
        encodings = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=256)
        encodings = {k: v.to(device) for k, v in encodings.items()}
        with torch.no_grad():
            outputs = self.model(encodings["input_ids"], encodings["attention_mask"], encodings["token_type_ids"])
            if len(outputs.shape) == 0:
                outputs = outputs.unsqueeze(0)
            probs = torch.stack([1-outputs, outputs], dim=1)
        return probs.cpu().numpy()
# èƒŒæ™¯è³‡æ–™å¯ä»¥ä½¿ç”¨å°æ¨£æœ¬å³å¯
background_data = [
    "ä»Šå¤©ä¸‹ç­å¾Œæœ‰ç©ºå—ï¼Ÿæƒ³ç´„ä½ å»é€›é€›æ–°é–‹çš„å¤œå¸‚ï¼Œè½èªªå¾ˆå¥½åƒã€‚",
    "ä½ çŸ¥é“æ€éº¼ç”¨åœ–å½¢è¨ˆç®—é«”ç©å—ï¼Ÿæˆ‘æ‰¾äº†å¹¾å€‹ä¾‹é¡Œæˆ‘å€‘ä¸€èµ·ç·´ã€‚",
    "æ€¡è±ï¼Œè¨­è¨ˆéƒ¨äº¤ä¾†çš„ææ¡ˆä¸å¤ªç¬¦åˆè¦æ ¼ï¼Œä½ å¹«å¿™è¯çµ¡ä¸€ä¸‹èª¿æ•´å­—å‹èˆ‡é…è‰²è¦ç¯„ã€‚",
    "æ­å–œæ‚¨ï¼åœ¨æˆ‘å€‘çš„å¹´ç”¨æˆ¶å›é¥‹æ´»å‹•ä¸­ï¼Œæ‚¨æŠ½ä¸­ä¸€å°ï¼è«‹åœ¨å°æ™‚å…§å®Œæˆç™»è¨˜æ‰‹çºŒé ˜å–è£œç¹³é€¾æœŸå°‡æ”¶å–æ‰‹çºŒè²»ã€‚",
    "æ‚¨ä¿¡ç”¨å¡å­˜åœ¨é«˜é£é™©æ¶ˆè´¹è®°å½•ï¼Œéœ€æ”¯ä»˜å®‰å…¨ä¿éšœè´¹å…ƒï¼Œè¯¦æƒ…è¯·è®¿é—®",
    "æ‚¨å¥½æˆ‘å€‘æ˜¯å®˜æ–¹å®¢æœï¼Œç™¼ç¾æ‚¨çš„å¸³æˆ¶æœ‰ç•°å¸¸è¡Œç‚ºï¼Œè«‹å›è¦†æ‚¨çš„å¯†ç¢¼åŠæ‰‹æ©Ÿè™Ÿç¢¼ä»¥ä¾¿å”åŠ©è™•ç†ã€‚",
    "æ‚¨å¥½ï¼Œä¿¡ç”¨å¡è´¦å•å¼‚å¸¸è¯·å°½å¿«æ”¯ä»˜æœªç»“æ¬¾é¡¹å…ƒï¼Œå®¢æœçƒ­çº¿",
    "æ‚¨å¥½ï¼Œæ‚¨çš„æ‰‹æœºå¥—é¤å¼‚å¸¸ï¼Œè¯·æ”¯ä»˜è¡¥ç¼´è´¹ç”¨å…ƒï¼Œå¾®ä¿¡å®¢æœã€‚",
    "æ‚¨çš„æ‰‹æ©Ÿå·²ä¸­æ¯’ï¼Œæ‰€æœ‰æ‡‰ç”¨ç¨‹å¼æœ‰è¢«é ç«¯æ“æ§çš„å±éšªï¼Œè«‹é¦¬ä¸Šæƒæä¸¦åˆªé™¤å¯ç–‘è»Ÿé«”ï¼Œé»æ­¤ä¸‹è¼‰ã€‚",
    "æƒ³èµ·ä½ ç¬¬ä¸€æ¬¡è‡ªå·±æ­é£›æ©Ÿé‚£å¤©ï¼Œåª½åª½å…¶å¯¦è¶…ç·Šå¼µçš„ï¼Œé‚„å·å“­äº†ä¸€ä¸‹ã€‚",
    "æˆ‘ä¸æ˜¯ä½ æ“æ§çš„æ£‹å­ã€‚",
    "æˆ‘ç¾åœ¨çš„ç‹€æ…‹å°±æ˜¯ä¸€å€‹æœƒå‘¼å¸çš„å¤±èª¤",
    "æ“ å…¬è»Šçš„æ™‚å€™è¢«å¤¾åˆ°èƒŒåŒ…ï¼Œå¾Œé¢å¤§å¬¸é‚„ä¸€ç›´å‚¬æˆ‘ä¸‹è»Š",
    "æ”¿åºœå…¬å‘Šæ‚¨ç¬¦åˆè€äººç¦åˆ©æ´¥è²¼è³‡æ ¼ï¼Œè«‹é»æ“Š",
    "æœ€è¿‘æœ‰ä»€éº¼å¥½è½çš„éŸ³æ¨‚æ¨è–¦å—ï¼Ÿæˆ‘æƒ³æ›æ›å£å‘³ã€‚",
    "æœ‰äººèªªå°åŒ—ä¿¡ç¾©å•†åœˆå¤œæ™šæœ‰äººç©¿å¤è£é€›è¡—ï¼Œåƒç©¿è¶Šæ™‚ç©ºã€‚",
    "æœ¬è¡Œç³»çµ±å‡ç´šéœ€é‡æ–°é©—è­‰ç”¨æˆ¶è³‡æ–™ï¼Œè«‹æ–¼å°æ™‚å…§å®Œæˆèº«ä»½ç¢ºèªç¨‹åºï¼Œå¦å‰‡å°‡æš«æ™‚å‡çµå¸³æˆ¶æ‰€æœ‰äº¤æ˜“åŠŸèƒ½ã€‚",
    "æ°´åˆ©ç½²é€šçŸ¥ï¼Œæœˆåˆå°‡é€²è¡Œè‡ªä¾†æ°´ç®¡ç·šä¾‹è¡Œç¶­ä¿®ï¼Œå¯èƒ½çŸ­æš«åœæ°´ã€‚",
    "ç·Šæ€¥é€šçŸ¥æ‚¨æœ‰ä¸€ç­†é˜²ç–«è£œåŠ©æ¬¾å¾…é ˜å–ï¼Œè«‹é»æ“Šé€£çµä¸Šå‚³èº«ä»½è­‰ç…§ç‰‡åŠæ‰‹æ©Ÿé©—è­‰ç¢¼ä»¥å®Œæˆç”³è«‹ã€‚ç¶²å€",
    "è¦ªæ„›çš„ç”¨æˆ¶ï¼Œæ‚¨çš„é›»ä¿¡åˆç´„å³å°‡åˆ°æœŸï¼Œè«‹é»æ“Šé€£çµç¢ºèªçºŒç´„ä¸¦äº«å—å„ªæƒ æ–¹æ¡ˆï¼Œé€¾æœŸå°‡è‡ªå‹•çµ‚æ­¢æœå‹™ã€‚"
]



def clean_text(text):
    text = re.sub(r"https?://\S+", "", text)
    text = re.sub(r"[a-zA-Z0-9:/.%\-_=+]{4,}", "", text)
    text = re.sub(r"\+?\d[\d\s\-]{5,}", "", text)
    text = re.sub(r"[^ä¸€-é¾¥ã€‚ï¼Œï¼ï¼Ÿã€]", "", text)
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", text)
    cleaned = "ã€‚".join(sentences[:4])
    return cleaned[:300]

def suspicious_tokens(text, explainer, top_k=5):
    try:
        shap_values = explainer.shap_values([text])
        tokens = tokenizer.tokenize(text)
        scores = shap_values[0][0]

        token_score = list(zip(tokens, scores))
        token_score.sort(key=lambda x: abs(x[1]), reverse=True)
        keywords = [t for t, s in token_score if len(t.strip()) > 1][:top_k]
        return keywords
    except Exception as e:
        print("âš  SHAP å¤±æ•—ï¼Œå•Ÿç”¨ fallback:", e)
        fallback = ["ç¹³è²»", "çµ‚æ­¢", "é€¾æœŸ", "é™æ™‚", "é©—è­‰ç¢¼"]
        return [kw for kw in fallback if kw in text]

model, tokenizer = load_model_and_tokenizer()
model.eval()

wrapped_model = ModelWrapper(model, tokenizer)

background_data = np.array(background_data)

explainer = shap.KernelExplainer(wrapped_model, background_data)
def analyze_text(text):
    cleaned_text = clean_text(text)

    result = predict_single_sentence(model, tokenizer, cleaned_text)
    label = result["label"]
    prob = result["prob"]
    risk = result["risk"]

    suspicious = suspicious_tokens(cleaned_text, explainer)

    highlighted_text = highlight_keywords(cleaned_text, suspicious)

    # ----------- é¡¯ç¤ºæ¨è«–è³‡è¨Šï¼ˆå¾Œç«¯çµ‚ç«¯æ©Ÿï¼‰ -----------
    print(f"\nğŸ“© è¨Šæ¯å…§å®¹ï¼š{text}")
    print(f"âœ… é æ¸¬çµæœï¼š{label}")  
    print(f"ğŸ“Š ä¿¡å¿ƒå€¼ï¼š{round(prob*100, 2)}")
    print(f"âš ï¸ é¢¨éšªç­‰ç´šï¼š{risk}")
    print(f"å¯ç–‘é—œéµå­—æ“·å–: { [str(s).strip() for s in suspicious if isinstance(s, str) and len(s.strip()) > 1]}")

    return {
        "status": label,
        "confidence": round(prob * 100, 2),
        "suspicious_keywords":  suspicious,
        "highlighted_text": highlighted_text 
    }

def highlight_keywords(text, keywords):
    for word in keywords:
        text = text.replace(word, f"<span class='highlight'>{word}</span>")
    return text

def analyze_image(file_bytes):
    image = Image.open(io.BytesIO(file_bytes))
    image_np = np.array(image)
    results = reader.readtext(image_np)
    
    text = ' '.join([res[1] for res in results]).strip()
    
    if not text:
        return{
            "status" : "ç„¡æ³•è¾¨è­˜æ–‡å­—",
            "confidence" : 0.0,
            "suspicious_keywords" : ["åœ–ç‰‡ä¸­ç„¡å¯è¾¨è­˜çš„ä¸­æ–‡è‹±æ–‡"],
            "highlighted_text": "ç„¡æ³•è¾¨è­˜å¯ç–‘å…§å®¹"
        }
    return analyze_text(text)