
# å¼•å…¥é‡è¦å¥—ä»¶Import Library
# PyTorch ä¸»æ¨¡çµ„ï¼Œå’ŒTensorflowå¾ˆåƒ 
# å…±é€šé»ï¼šéƒ½æ˜¯æ·±åº¦å­¸ç¿’æ¡†æ¶ï¼Œæ”¯æ´å»ºæ§‹ç¥ç¶“ç¶²è·¯ã€è¨“ç·´èˆ‡æ¨è«–ï¼Œéƒ½æ”¯æ´GPUåŠ é€Ÿã€è¼‰å…¥æ¨¡å‹ï¼Œå’Œè™•ç†tensorç­‰ã€‚
# æ“ä½œæ¯”è¼ƒç›´è¦ºï¼Œæ¥è¿‘Pythonæœ¬èº«çš„é¢¨æ ¼ï¼Œå‹•æ…‹åœ–æ¶æ§‹(æ¯ä¸€æ¬¡forwardéƒ½å³æ™‚è¨ˆç®—)ï¼Œæ›´å®¹æ˜“é™¤éŒ¯ã€å¿«é€Ÿè¿­ä»£ï¼Œåœ¨ç ”ç©¶é ˜åŸŸéå¸¸æµè¡Œã€‚
# reæ˜¯Pythonå…§å»ºçš„æ­£å‰‡è¡¨ç¤ºå¼(regular expression)æ¨¡çµ„ï¼Œåœ¨é€™å°ˆæ¡ˆä¸­ç”¨ä¾†"ç”¨é—œéµè¦å‰‡ç¯©é¸æ–‡å­—å…§å®¹"ã€‚
# requestsæ˜¯ä¸€å€‹éå¸¸å¥½ç”¨çš„ HTTP è«‹æ±‚å¥—ä»¶ï¼Œèƒ½è®“ä½ å¾Pythonç™¼é€GET/POSTè«‹æ±‚ï¼Œåœ¨å°ˆæ¡ˆä¸­ç”¨ä¾†å¾Google Driveä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ(model.pth)ã€‚
# BertTokenizer:å¾Hugging Faceçš„transformerså¥—ä»¶è¼‰å…¥ä¸€å€‹å°ˆç”¨çš„ã€Œåˆ†è©å™¨ï¼ˆTokenizerï¼‰ã€ã€‚
# pip install shap

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

from lime.lime_text import LimeTextExplainer
import torch                
import re
import easyocr
import io
import numpy as np

from PIL import Image
from huggingface_hub import hf_hub_download
from transformers import BertTokenizer



reader = easyocr.Reader(['ch_tra', 'en'], gpu=torch.cuda.is_available())
# è¨­å®šè£ç½®ï¼ˆGPU å„ªå…ˆï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# é è¨­æ¨¡å‹èˆ‡ tokenizer ç‚º Noneï¼Œç›´åˆ°é¦–æ¬¡è«‹æ±‚æ‰è¼‰å…¥ï¼ˆå»¶é²è¼‰å…¥ï¼‰
model = None
tokenizer = None
# âœ… å»¶é²è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer
def load_model_and_tokenizer():
    global model, tokenizer
    if os.path.exists("model.pth"):
        model_path = "model.pth"
    else:
        model_path = hf_hub_download(repo_id="Bennie12/Bert-Lstm-Cnn-ScamDetecter", filename="model.pth")
    # åŒ¯å…¥æ¨¡å‹æ¶æ§‹ï¼ˆé¿å…åœ¨æ¨¡çµ„åˆå§‹åŒ–éšæ®µå°±å ç”¨å¤§é‡è¨˜æ†¶é«”ï¼‰
    from AI_Model_architecture import BertLSTM_CNN_Classifier

    # è¼‰å…¥æ¨¡å‹æ¶æ§‹èˆ‡åƒæ•¸ï¼Œåˆå§‹åŒ–æ¨¡å‹æ¶æ§‹ä¸¦è¼‰å…¥è¨“ç·´æ¬Šé‡
    model = BertLSTM_CNN_Classifier()
    
    # é€™è¡Œçš„åŠŸèƒ½æ˜¯ï¼šã€Œå¾ model_pathæŠŠ.pth æ¬Šé‡æª”æ¡ˆè®€é€²ä¾†ï¼Œè¼‰å…¥é€²æ¨¡å‹è£¡ã€ã€‚
    # model.load_state_dict(...)æŠŠä¸Šé¢è¼‰å…¥çš„æ¬Šé‡ã€Œå¥—é€²æ¨¡å‹æ¶æ§‹è£¡ã€
    # torch.load(...)è¼‰å…¥.pth æ¬Šé‡æª”æ¡ˆï¼Œæœƒè®Šæˆä¸€ä»½ Python å­—å…¸
    # map_location=deviceæŒ‡å®šæ¨¡å‹è¼‰å…¥åˆ° CPU é‚„æ˜¯ GPUï¼Œé¿å…å ±éŒ¯
    model.load_state_dict(torch.load(model_path, map_location=device))
    
    model.to(device)
    
    # é€™æ˜¯PyTorchä¸­çš„ã€Œæ¨è«–æ¨¡å¼ã€è¨­å®š
    # model.eval()æ¨¡å‹è™•æ–¼æ¨è«–ç‹€æ…‹ï¼ˆé—œæ‰ Dropout ç­‰éš¨æ©Ÿæ“ä½œï¼‰
    # åªè¦æ˜¯ç”¨ä¾†ã€Œé æ¸¬ã€è€Œä¸æ˜¯è¨“ç·´ï¼Œä¸€å®šè¦åŠ  .eval()ï¼
    model.eval()

    # åˆå§‹åŒ– tokenizer(ä¸è¦å¾ build_bert_inputs ä¸­å–)
    # è¼‰å…¥é è¨“ç·´å¥½çš„CKIPä¸­æ–‡BERTåˆ†è©å™¨
    # èƒ½æŠŠä¸­æ–‡å¥å­è½‰æˆ BERT æ¨¡å‹éœ€è¦çš„ input æ ¼å¼ï¼ˆinput_ids, attention_mask, token_type_idsï¼‰
    tokenizer = BertTokenizer.from_pretrained("ckiplab/bert-base-chinese", use_fast=False)  # âœ… å¼·åˆ¶ä½¿ç”¨é fast tokenizer


    return model, tokenizer

all_preds = []
all_labels = []

# é æ¸¬å–®ä¸€å¥å­çš„åˆ†é¡çµæœï¼ˆè©é¨™ or æ­£å¸¸ï¼‰
# model: è¨“ç·´å¥½çš„PyTorchæ¨¡å‹
# tokenizer: åˆ†è©å™¨ï¼Œè² è²¬æŠŠä¸­æ–‡è½‰æˆ BERT èƒ½è™•ç†çš„æ•¸å€¼æ ¼å¼
# sentence: ä½¿ç”¨è€…è¼¸å…¥çš„æ–‡å­—å¥å­
# max_len: é™åˆ¶æœ€å¤§è¼¸å…¥é•·åº¦ï¼ˆé è¨­ 256 å€‹ tokenï¼‰
def predict_single_sentence(model, tokenizer, sentence, max_len=256):
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ä½¿ç”¨ with torch.no_grad()ï¼Œä»£è¡¨é€™æ®µç¨‹å¼ã€Œä¸éœ€è¦è¨˜éŒ„æ¢¯åº¦ã€
    # é€™æ¨£å¯ä»¥åŠ é€Ÿæ¨è«–ä¸¦ç¯€çœè¨˜æ†¶é«”
    with torch.no_grad():
         # ----------- æ–‡å­—å‰è™•ç†ï¼šæ¸…æ´—è¼¸å…¥å¥å­ -----------
        sentence = re.sub(r"\s+", "", sentence)  # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—å…ƒï¼ˆç©ºæ ¼ã€æ›è¡Œç­‰ï¼‰
        sentence = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚ï¼Œï¼ï¼Ÿ:/._-]", "", sentence)
        # ä¿ç•™å¸¸è¦‹ä¸­æ–‡å­—ã€è‹±æ•¸å­—èˆ‡æ¨™é»ç¬¦è™Ÿï¼Œå…¶ä»–å¥‡æ€ªç¬¦è™Ÿéƒ½ç§»é™¤
        # ----------- ä½¿ç”¨ BERT Tokenizer å°‡å¥å­ç·¨ç¢¼ -----------
        encoded = tokenizer(sentence,
                            return_tensors="pt",       # å›å‚³ PyTorch tensor æ ¼å¼ï¼ˆé è¨­æ˜¯ numpy æˆ– listï¼‰
                            truncation=True,           # è¶…éæœ€å¤§é•·åº¦å°±æˆªæ–·
                            padding="max_length",      # ä¸è¶³æœ€å¤§é•·åº¦å‰‡è£œç©ºç™½ï¼ˆPAD tokenï¼‰
                            max_length=max_len)        # è¨­å®šæœ€å¤§é•·åº¦ç‚º 256
        # æŠŠ tokenizer å›å‚³çš„è³‡æ–™é€é€²æ¨¡å‹å‰ï¼Œto(device)è½‰åˆ°æŒ‡å®šçš„è£ç½®ï¼ˆGPU or CPUï¼‰
        input_ids = encoded["input_ids"].to(device)
        attention_mask = encoded["attention_mask"].to(device)
        token_type_ids = encoded["token_type_ids"].to(device)
        # ----------- æ¨¡å‹æ¨è«–ï¼šè¼¸å‡ºè©é¨™çš„æ©Ÿç‡å€¼ -----------
        output = model(input_ids, attention_mask, token_type_ids)# å›å‚³çš„æ˜¯ä¸€å€‹æ©Ÿç‡å€¼ï¼ˆfloatï¼‰
        prob = output.item()  # å¾ tensor å–å‡ºç´”æ•¸å­—ï¼Œä¾‹å¦‚ 0.86
        label = int(prob > 0.5)  # å¦‚æœæ©Ÿç‡ > 0.5ï¼Œæ¨™ç‚ºã€Œè©é¨™ã€ï¼ˆ1ï¼‰ï¼Œå¦å‰‡ç‚ºã€Œæ­£å¸¸ã€ï¼ˆ0ï¼‰
        # ----------- æ ¹æ“šæ©Ÿç‡é€²è¡Œé¢¨éšªåˆ†ç´š -----------
        if prob > 0.9:
            risk = "ğŸ”´ é«˜é¢¨éšªï¼ˆæ¥µå¯èƒ½æ˜¯è©é¨™ï¼‰"
        elif prob > 0.5:
            risk = "ğŸŸ¡ ä¸­é¢¨éšªï¼ˆå¯ç–‘ï¼‰"
        else:
            risk = "ğŸŸ¢ ä½é¢¨éšªï¼ˆæ­£å¸¸ï¼‰"
        # ----------- æ ¹æ“š label åˆ¤æ–·æ–‡å­—çµæœ -----------
        pre_label ='è©é¨™'if label == 1 else 'æ­£å¸¸'
        
        # çµ„æˆä¸€å€‹ Python å­—å…¸ï¼ˆå°æ‡‰ API çš„ JSON è¼¸å‡ºæ ¼å¼ï¼‰
        return {
        "label" : pre_label,                  # é æ¸¬åˆ†é¡ï¼ˆ"è©é¨™" or "æ­£å¸¸"ï¼‰
        "prob" : prob, # é æ¸¬åˆ†é¡ï¼ˆ"è©é¨™" or "æ­£å¸¸"ï¼‰  
        "risk" : risk     # ç”¨é¢¨éšªåˆ†ç´šç•¶ä½œ"å¯ç–‘æç¤º"æ”¾é€² listï¼ˆåç¨±ç‚º suspicious_keywordsï¼‰
    }

# analyze_text(text)å°æ‡‰app.pyç¬¬117è¡Œ
# é€™å€‹å‡½å¼æ˜¯ã€Œå°å¤–çš„ç°¡åŒ–ç‰ˆæœ¬ã€ï¼šè¼¸å…¥ä¸€å¥æ–‡å­— â†’ å›å‚³è©é¨™åˆ¤å®šçµæœ
# ç”¨åœ¨ä¸»ç¨‹å¼æˆ– FastAPI å¾Œç«¯ä¸­ï¼Œæ˜¯æ•´å€‹æ¨¡å‹é æ¸¬æµç¨‹çš„å…¥å£é»

# ----------- LIMEå¯ç–‘è©å¥æ“·å– -----------
def suspicious_tokens(model, tokenizer, text, top_k=4):
    print("\nğŸ” [suspicious_tokens] å‡½å¼è¢«å‘¼å«")
    print(f"ğŸ“¥ å‚³å…¥æ–‡å­—å…§å®¹ï¼š{text}")
    print(f"ğŸ“¥ è³‡æ–™å‹åˆ¥ï¼š{type(text)}")

    if not isinstance(text, str) or not text.strip():
        print("âŒ è­¦å‘Šï¼šè¼¸å…¥ä¸æ˜¯åˆæ³•æ–‡å­—ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨")
        return []
    if len(text.strip()) < 4:  # âœ… å¦‚æœè¼¸å…¥å¤ªçŸ­ï¼Œç›´æ¥è·³éæ“¾å‹•ï¼Œé¿å…éŒ¯èª¤
        print("âš ï¸ è­¦å‘Šï¼šæ–‡å­—é•·åº¦éçŸ­ï¼ˆå°‘æ–¼4å­—ï¼‰ï¼Œè·³éLIMEåˆ†æ")
        return []
    
    def predict_proba(texts):

        encoding = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=256)
        encoding = {k: v.to(device) for k, v in encoding.items()}
        with torch.no_grad():
            outputs = model(encoding["input_ids"], encoding["attention_mask"], encoding["token_type_ids"])
            probs = torch.stack([1 - outputs, outputs], dim=1)

        return probs.cpu().numpy()

    class_names = ['æ­£å¸¸', 'è©é¨™']
    explainer = LimeTextExplainer(
    class_names=class_names,
    split_expression='',  # âœ… æ¯å­—ç‚ºæ“¾å‹•å–®ä½ï¼ˆé©åˆä¸­æ–‡ï¼‰
    bow=False             # âœ… ä¿ç•™èªåºèˆ‡ä¸Šä¸‹æ–‡èªç¾©
)
    try:
        explanation = explainer.explain_instance(
            text, predict_proba,
            num_features=top_k,
            labels=[1],
            num_samples=700
        )
        keyword_scores = explanation.as_list(label=1)
        keywords = [word for word, score in keyword_scores]
        print(f"é—œéµå­—é•·:{keywords}")
        return keywords
    except Exception as e:
        print(f"âš ï¸ LIME æ“¾å‹•åˆ†æå¤±æ•—ï¼š{e}")
        return []


    
def analyze_text(text):
    model, tokenizer = load_model_and_tokenizer()
    model.eval()

    # é æ¸¬æ¨™ç±¤èˆ‡ä¿¡å¿ƒåˆ†æ•¸
    result = predict_single_sentence(model, tokenizer, text)
    label = result["label"]
    prob = result["prob"]
    risk = result["risk"]

    # æ“·å–å¯ç–‘è©
    suspicious = suspicious_tokens(model, tokenizer, text)
    
    # ----------- é¡¯ç¤ºæ¨è«–è³‡è¨Šï¼ˆå¾Œç«¯çµ‚ç«¯æ©Ÿï¼‰ -----------
    print(f"\nğŸ“© è¨Šæ¯å…§å®¹ï¼š{text}")
    print(f"âœ… é æ¸¬çµæœï¼š{label}")  
    print(f"ğŸ“Š ä¿¡å¿ƒå€¼ï¼š{round(prob*100, 2)}")
    print(f"âš ï¸ é¢¨éšªç­‰ç´šï¼š{risk}")
    print(f"å¯ç–‘é—œéµå­—æ“·å–: {[str(s) for s in suspicious]}")
    
    return {
        "status": label,
        "confidence": round(prob * 100, 2),
        "suspicious_keywords": [str(s) for s in suspicious]
    }

def analyze_image(file_bytes):
    image = Image.open(io.BytesIO(file_bytes))
    image_np = np.array(image)
    results = reader.readtext(image_np)
    
    text = ' '.join([res[1] for res in results]).strip()
    
    if not text:
        return{
            "status" : "ç„¡æ³•è¾¨è­˜æ–‡å­—",
            "confidence" : 0.0,
            "suspicious_keywords" : ["åœ–ç‰‡ä¸­ç„¡å¯è¾¨è­˜çš„ä¸­æ–‡è‹±æ–‡"]
        }
    return analyze_text(text)