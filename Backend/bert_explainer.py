#pip install transformers
#pip install torch         //transformers å¥—ä»¶éœ€è¦
#pip install scikit-learn
#pip install transformers torch
#
# bert_explainer.py


#å¼•å…¥é‡è¦å¥—ä»¶Import Library
import torch                            #   PyTorch ä¸»æ¨¡çµ„               
import torch
from AI_Model_architecture import BertLSTM_CNN_Classifier, BertPreprocessor #	ç”¨ä¾†ç”¢ç”Ÿæ¨¡æ“¬è³‡æ–™
from transformers import BertTokenizer
import re
#BertTokenizer	æŠŠæ–‡å­—å¥å­è½‰æ›æˆ BERT æ ¼å¼çš„ token IDï¼Œä¾‹å¦‚ [CLS] ä»Šå¤© å¤©æ°£ ä¸éŒ¯ [SEP] â†’ [101, 1234, 5678, ...]

# è£ç½®é¸æ“‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åˆå§‹åŒ–æ¨¡å‹æ¶æ§‹ä¸¦è¼‰å…¥è¨“ç·´æ¬Šé‡
model = BertLSTM_CNN_Classifier()
model.load_state_dict(torch.load("model.pth", map_location=device))
model.to(device)
model.eval()

# åˆå§‹åŒ– tokenizerï¼ˆä¸è¦å¾ build_bert_inputs ä¸­å–ï¼‰
tokenizer = BertTokenizer.from_pretrained("ckiplab/bert-base-chinese")

all_preds = []
all_labels = []

def predict_single_sentence(model, tokenizer, sentence, max_len=256):
    model.eval()
    with torch.no_grad():
        # æ¸…æ´—æ–‡å­—
        sentence = re.sub(r"\s+", "", sentence)
        sentence = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚ï¼Œï¼ï¼Ÿ]", "", sentence)

        # ç·¨ç¢¼
        encoded = tokenizer(sentence,
                            return_tensors="pt",
                            truncation=True,
                            padding="max_length",
                            max_length=max_len)

        input_ids = encoded["input_ids"].to(device)
        attention_mask = encoded["attention_mask"].to(device)
        token_type_ids = encoded["token_type_ids"].to(device)

        # é æ¸¬
        output = model(input_ids, attention_mask, token_type_ids)
        prob = output.item()
        label = int(prob > 0.5)

        # é¢¨éšªåˆ†ç´šé¡¯ç¤º
        if prob > 0.9:
            risk = "ğŸ”´ é«˜é¢¨éšªï¼ˆæ¥µå¯èƒ½æ˜¯è©é¨™ï¼‰"
        elif prob > 0.5:
            risk = "ğŸŸ¡ ä¸­é¢¨éšªï¼ˆå¯ç–‘ï¼‰"
        else:
            risk = "ğŸŸ¢ ä½é¢¨éšªï¼ˆæ­£å¸¸ï¼‰"

        pre_label = ""
        if label == 1:
            pre_label = 'è©é¨™'
        else:
            pre_label = 'æ­£å¸¸'
   
        print(f"\nğŸ“© è¨Šæ¯å…§å®¹ï¼š{sentence}")
        print(f"âœ… é æ¸¬çµæœï¼š{'è©é¨™' if label == 1 else 'æ­£å¸¸'}")
        print(f"ğŸ“Š ä¿¡å¿ƒå€¼ï¼š{round(prob*100, 2)}")
        print(f"âš ï¸ é¢¨éšªç­‰ç´šï¼š{risk}")
        return pre_label, prob, risk


def analyze_text(text):
    label, prob, risk = predict_single_sentence(model, tokenizer, text)
    return {
        "status": label,
        "confidence": round(prob*100, 2),  
        "suspicious_keywords": [risk]
    }

