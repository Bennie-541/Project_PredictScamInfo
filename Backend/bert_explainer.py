import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import torch                
import re
import easyocr
import io
import numpy as np
from PIL import Image
from huggingface_hub import hf_hub_download
from transformers import BertTokenizer
from AI_Model_architecture import BertLSTM_CNN_Classifier
from lime.lime_text import LimeTextExplainer

# OCR æ¨¡çµ„
reader = easyocr.Reader(['ch_tra', 'en'], gpu=torch.cuda.is_available())

# è¨­å®šè£ç½®ï¼ˆGPU å„ªå…ˆï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer
def load_model_and_tokenizer():
    global model, tokenizer

    if os.path.exists("model.pth"):
        print("âœ… å·²æ‰¾åˆ° model.pth è¼‰å…¥æ¨¡å‹")
        model_path = "model.pth"
    else:
        print("ğŸš€ æœªæ‰¾åˆ° model.pth")
        model_path = hf_hub_download(repo_id="Bennie12/Bert-Lstm-Cnn-ScamDetecter", filename="model.pth")

    model = BertLSTM_CNN_Classifier()
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()

    tokenizer = BertTokenizer.from_pretrained("ckiplab/bert-base-chinese", use_fast=False)

    return model, tokenizer

model, tokenizer = load_model_and_tokenizer()
model.eval()

# é æ¸¬å–®ä¸€å¥å­çš„åˆ†é¡çµæœ
def predict_single_sentence(model, tokenizer, sentence, max_len=256):
    sentence = re.sub(r"\s+", "", sentence)
    sentence = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚ï¼Œï¼ï¼Ÿ:/._-]", "", sentence)

    encoded = tokenizer(sentence, return_tensors="pt", truncation=True, padding="max_length", max_length=max_len)
    encoded = {k: v.to(device) for k, v in encoded.items()}

    with torch.no_grad():
        output = model(encoded["input_ids"], encoded["attention_mask"], encoded["token_type_ids"])
        prob = torch.sigmoid(output).item()
        label = int(prob > 0.5)

    risk = "ğŸŸ¢ ä½é¢¨éšªï¼ˆæ­£å¸¸ï¼‰"
    if prob > 0.9:
        risk = "ğŸ”´ é«˜é¢¨éšªï¼ˆæ¥µå¯èƒ½æ˜¯è©é¨™ï¼‰"
    elif prob > 0.5:
        risk = "ğŸŸ¡ ä¸­é¢¨éšªï¼ˆå¯ç–‘ï¼‰"

    pre_label = 'è©é¨™' if label == 1 else 'æ­£å¸¸'
    
    return {
        "label": pre_label,
        "prob": prob,
        "risk": risk
    }

# æä¾› LIME ç”¨çš„ predict_proba
def predict_proba(texts):
    # tokenizer æ‰¹æ¬¡è™•ç†
    encoded = tokenizer(
        texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=256
    )

    # ç§»å‹•åˆ° GPU æˆ– CPU
    encoded = {k: v.to(device) for k, v in encoded.items()}

    with torch.no_grad():
        outputs = model(encoded["input_ids"], encoded["attention_mask"], encoded["token_type_ids"])
        # outputs shape: (batch_size,)
        probs = torch.sigmoid(outputs).cpu().numpy()

    # è½‰æˆ LIME æ ¼å¼ï¼š(N, 2)
    probs_2d = np.vstack([1-probs, probs]).T
    return probs_2d

    # ç§»å‹•åˆ° GPU æˆ– CPU
    encoded = {k: v.to(device) for k, v in encoded.items()}

    with torch.no_grad():
        outputs = model(encoded["input_ids"], encoded["attention_mask"], encoded["token_type_ids"])
        # outputs shape: (batch_size,)
        probs = torch.sigmoid(outputs).cpu().numpy()

    # è½‰æˆ LIME æ ¼å¼ï¼š(N, 2)
    probs_2d = np.vstack([1-probs, probs]).T
    return probs_2d

# åˆå§‹åŒ– LIME explainer
class_names = ['æ­£å¸¸', 'è©é¨™']
lime_explainer = LimeTextExplainer(class_names=class_names)

# æ“·å–å¯ç–‘è©å½™ (æ”¹ç”¨ LIME)
def suspicious_tokens(text, explainer=lime_explainer, top_k=5):
    try:
        explanation = explainer.explain_instance(text, predict_proba, num_features=top_k, num_samples=500 )
        keywords = [word for word, weight in explanation.as_list()]
        return keywords
    except Exception as e:
        print("âš  LIME å¤±æ•—ï¼Œå•Ÿç”¨ fallback:", e)
        fallback = ["ç¹³è²»", "çµ‚æ­¢", "é€¾æœŸ", "é™æ™‚", "é©—è­‰ç¢¼"]
        return [kw for kw in fallback if kw in text]

# æ–‡å­—æ¸…ç†
def clean_text(text):
    text = re.sub(r"https?://\S+", "", text)
    text = re.sub(r"[a-zA-Z0-9:/.%\-_=+]{4,}", "", text)
    text = re.sub(r"\+?\d[\d\s\-]{5,}", "", text)
    text = re.sub(r"[^ä¸€-é¾¥ã€‚ï¼Œï¼ï¼Ÿã€]", "", text)
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", text)
    cleaned = "ã€‚".join(sentences[:4])
    return cleaned[:300]

# é«˜äº®é¡¯ç¤º
def highlight_keywords(text, keywords):
    for word in keywords:
        text = text.replace(word, f"<span class='highlight'>{word}</span>")
    return text

# æ–‡å­—åˆ†æä¸»æµç¨‹
def analyze_text(text):
    cleaned_text = clean_text(text)
    result = predict_single_sentence(model, tokenizer, cleaned_text)
    label = result["label"]
    prob = result["prob"]
    risk = result["risk"]

    suspicious = suspicious_tokens(cleaned_text)
    highlighted_text = highlight_keywords(text, suspicious)

    print(f"\nğŸ“© è¨Šæ¯å…§å®¹ï¼š{text}")
    print(f"âœ… é æ¸¬çµæœï¼š{label}")  
    print(f"ğŸ“Š ä¿¡å¿ƒå€¼ï¼š{round(prob*100, 2)}")
    print(f"âš ï¸ é¢¨éšªç­‰ç´šï¼š{risk}")
    print(f"å¯ç–‘é—œéµå­—æ“·å–: {suspicious}")

    return {
        "status": label,
        "confidence": round(prob * 100, 2),
        "suspicious_keywords": suspicious,
        "highlighted_text": highlighted_text 
    }

# åœ–ç‰‡ OCR åˆ†æ
def analyze_image(file_bytes):
    image = Image.open(io.BytesIO(file_bytes))
    image_np = np.array(image)
    results = reader.readtext(image_np)
    
    text = ' '.join([res[1] for res in results]).strip()
    
    if not text:
        return {
            "status" : "ç„¡æ³•è¾¨è­˜æ–‡å­—",
            "confidence" : 0.0,
            "suspicious_keywords" : ["åœ–ç‰‡ä¸­ç„¡å¯è¾¨è­˜çš„ä¸­æ–‡è‹±æ–‡"],
            "highlighted_text": "ç„¡æ³•è¾¨è­˜å¯ç–‘å…§å®¹"
        }
    return analyze_text(text)
