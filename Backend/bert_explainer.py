#pip install transformers
#pip install torch         //transformers å¥—ä»¶éœ€è¦
#pip install scikit-learn
#pip install transformers torch

# å¼•å…¥é‡è¦å¥—ä»¶Import Library
# PyTorch ä¸»æ¨¡çµ„ï¼Œå’ŒTensorflowå¾ˆåƒ 
# å…±é€šé»ï¼šéƒ½æ˜¯æ·±åº¦å­¸ç¿’æ¡†æ¶ï¼Œæ”¯æ´å»ºæ§‹ç¥ç¶“ç¶²è·¯ã€è¨“ç·´èˆ‡æ¨è«–ï¼Œéƒ½æ”¯æ´GPUåŠ é€Ÿã€è¼‰å…¥æ¨¡å‹ï¼Œå’Œè™•ç†tensorç­‰ã€‚
# æ“ä½œæ¯”è¼ƒç›´è¦ºï¼Œæ¥è¿‘Pythonæœ¬èº«çš„é¢¨æ ¼ï¼Œå‹•æ…‹åœ–æ¶æ§‹(æ¯ä¸€æ¬¡forwardéƒ½å³æ™‚è¨ˆç®—)ï¼Œæ›´å®¹æ˜“é™¤éŒ¯ã€å¿«é€Ÿè¿­ä»£ï¼Œåœ¨ç ”ç©¶é ˜åŸŸéå¸¸æµè¡Œã€‚
# reæ˜¯Pythonå…§å»ºçš„æ­£å‰‡è¡¨ç¤ºå¼(regular expression)æ¨¡çµ„ï¼Œåœ¨é€™å°ˆæ¡ˆä¸­ç”¨ä¾†"ç”¨é—œéµè¦å‰‡ç¯©é¸æ–‡å­—å…§å®¹"ã€‚
# requestsæ˜¯ä¸€å€‹éå¸¸å¥½ç”¨çš„ HTTP è«‹æ±‚å¥—ä»¶ï¼Œèƒ½è®“ä½ å¾Pythonç™¼é€GET/POSTè«‹æ±‚ï¼Œåœ¨å°ˆæ¡ˆä¸­ç”¨ä¾†å¾Google Driveä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ(model.pth)ã€‚
# BertTokenizer:å¾Hugging Faceçš„transformerså¥—ä»¶è¼‰å…¥ä¸€å€‹å°ˆç”¨çš„ã€Œåˆ†è©å™¨ï¼ˆTokenizerï¼‰ã€ã€‚
import torch                
import re
import os
import requests

from Backend.AI_Model_architecture import BertLSTM_CNN_Classifier 
from transformers import BertTokenizer


# é€™æ˜¯ä¸€å€‹å‡½å¼ï¼Œè®“ä½ è‡ªå‹•å¾ Google Drive æŠ“ä¸‹æ¨¡å‹æª”æ¡ˆ .pthã€‚
# file_id: ä½ å¾ Google Drive æ‹¿åˆ°çš„æ¨¡å‹ ID
# destination: ä½ è¦å„²å­˜çš„æœ¬åœ°æª”æ¡ˆè·¯å¾‘ï¼ˆä¾‹å¦‚ "model.pth"ï¼‰
def download_model_from_Gdrive(file_id, destination):#Model.pthé€£çµ(https://drive.google.com/file/d/19t6NlRFMc1i8bGtngRwIRtRcCmibdP9q/view?usp=drive_link)
    # urlè®Šæ•¸ç”¨ä¾†ç”¢ç”ŸçœŸæ­£çš„ã€Œç›´æ¥ä¸‹è¼‰é€£çµã€file_idé è¨­æ˜¯Googleé›²ç«¯è£¡çš„model.pthæª”æ¡ˆidï¼Œè®“ç¨‹å¼èƒ½ä¸‹è¼‰model.pth
    url = f"https://drive.google.com/uc?export=download&id={file_id}"  
    if not os.path.exists(destination):   # å¦‚æœæœ¬åœ°é‚„æ²’æœ‰é€™å€‹æª”æ¡ˆ â†’ æ‰ä¸‹è¼‰ï¼ˆé¿å…é‡è¤‡ï¼‰
        print("ğŸ“¥ Downloading model from Google Drive...")
        r = requests.get(url)             # ç”¨requestsç™¼é€GETè«‹æ±‚åˆ°Google Drive
        with open(destination, 'wb')as f: # æŠŠä¸‹è¼‰çš„æª”æ¡ˆå…§å®¹å¯«å…¥åˆ° model.pth æœ¬åœ°æª”æ¡ˆ
            f.write(r.content)
        print("âœ… Model downloaded.")     
    else:
        print("ğŸ“¦ Model already exists.")
        
# åœ¨ä½  load æ¨¡å‹ä¹‹å‰åŸ·è¡Œ
# é€™è¡Œæ˜¯ç‚ºäº†å–å¾—æ¨¡å‹å„²å­˜è·¯å¾‘ï¼Œç¢ºä¿å³ä½¿æ›ä¸åŒé›»è…¦æˆ–ä¸Šé›²éƒ¨ç½²ä¹Ÿèƒ½æ­£ç¢ºæ‰¾åˆ°æª”æ¡ˆã€‚
# ç°¡å–®è¬›ï¼šé€™è¡Œå°±æ˜¯ã€Œæ‰¾åˆ°é€™æ”¯ç¨‹å¼æ‰€åœ¨çš„è³‡æ–™å¤¾ï¼Œä¸¦æŒ‡å®šé‚£è£¡çš„ model.pth æª”æ¡ˆã€
# os.path.join(è³‡æ–™å¤¾,"model.pth")æŠŠè³‡æ–™å¤¾+æª”åã€Œå®‰å…¨åœ°åˆä½µã€ï¼Œè®Šæˆå®Œæ•´è·¯å¾‘(è·¨å¹³å°å…¼å®¹)
# __file__æ˜¯Pythonå…§å»ºè®Šæ•¸ï¼Œä»£è¡¨ã€Œç›®å‰é€™æ”¯ç¨‹å¼ç¢¼çš„æª”æ¡ˆè·¯å¾‘ã€
# os.path.dirname(__file__)å–å¾—é€™æ”¯.py æª”çš„ã€Œè³‡æ–™å¤¾è·¯å¾‘ã€
model_path = os.path.join(os.path.dirname(__file__), "model.pth")#åŠŸèƒ½èªªæ˜ï¼š

download_model_from_Gdrive("19t6NlRFMc1i8bGtngRwIRtRcCmibdP9q", model_path)

# è£ç½®é¸æ“‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åˆå§‹åŒ–æ¨¡å‹æ¶æ§‹ä¸¦è¼‰å…¥è¨“ç·´æ¬Šé‡
model = BertLSTM_CNN_Classifier()

# é€™è¡Œçš„åŠŸèƒ½æ˜¯ï¼šã€Œå¾ model_pathæŠŠ.pth æ¬Šé‡æª”æ¡ˆè®€é€²ä¾†ï¼Œè¼‰å…¥é€²æ¨¡å‹è£¡ã€ã€‚
# model.load_state_dict(...)æŠŠä¸Šé¢è¼‰å…¥çš„æ¬Šé‡ã€Œå¥—é€²æ¨¡å‹æ¶æ§‹è£¡ã€
# torch.load(...)è¼‰å…¥.pth æ¬Šé‡æª”æ¡ˆï¼Œæœƒè®Šæˆä¸€ä»½ Python å­—å…¸
# map_location=deviceæŒ‡å®šæ¨¡å‹è¼‰å…¥åˆ° CPU é‚„æ˜¯ GPUï¼Œé¿å…å ±éŒ¯
model.load_state_dict(torch.load(model_path, map_location=device))

model.to(device)

# é€™æ˜¯PyTorchä¸­çš„ã€Œæ¨è«–æ¨¡å¼ã€è¨­å®š
# model.eval()æ¨¡å‹è™•æ–¼æ¨è«–ç‹€æ…‹ï¼ˆé—œæ‰ Dropout ç­‰éš¨æ©Ÿæ“ä½œï¼‰
# åªè¦æ˜¯ç”¨ä¾†ã€Œé æ¸¬ã€è€Œä¸æ˜¯è¨“ç·´ï¼Œä¸€å®šè¦åŠ  .eval()ï¼
model.eval()

# åˆå§‹åŒ– tokenizer(ä¸è¦å¾ build_bert_inputs ä¸­å–)
# è¼‰å…¥é è¨“ç·´å¥½çš„CKIPä¸­æ–‡BERTåˆ†è©å™¨
# èƒ½æŠŠä¸­æ–‡å¥å­è½‰æˆ BERT æ¨¡å‹éœ€è¦çš„ input æ ¼å¼ï¼ˆinput_ids, attention_mask, token_type_idsï¼‰
tokenizer = BertTokenizer.from_pretrained("ckiplab/bert-base-chinese")

all_preds = []
all_labels = []

def predict_single_sentence(model, tokenizer, sentence, max_len=256):
    model.eval()
    with torch.no_grad():
        # æ¸…æ´—æ–‡å­—
        sentence = re.sub(r"\s+", "", sentence)
        sentence = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚ï¼Œï¼ï¼Ÿ]", "", sentence)

        # ç·¨ç¢¼
        encoded = tokenizer(sentence,
                            return_tensors="pt",
                            truncation=True,
                            padding="max_length",
                            max_length=max_len)

        input_ids = encoded["input_ids"].to(device)
        attention_mask = encoded["attention_mask"].to(device)
        token_type_ids = encoded["token_type_ids"].to(device)

        # é æ¸¬
        output = model(input_ids, attention_mask, token_type_ids)
        prob = output.item()
        label = int(prob > 0.5)

        # é¢¨éšªåˆ†ç´šé¡¯ç¤º
        if prob > 0.9:
            risk = "ğŸ”´ é«˜é¢¨éšªï¼ˆæ¥µå¯èƒ½æ˜¯è©é¨™ï¼‰"
        elif prob > 0.5:
            risk = "ğŸŸ¡ ä¸­é¢¨éšªï¼ˆå¯ç–‘ï¼‰"
        else:
            risk = "ğŸŸ¢ ä½é¢¨éšªï¼ˆæ­£å¸¸ï¼‰"

        pre_label = ""
        if label == 1:
            pre_label = 'è©é¨™'
        else:
            pre_label = 'æ­£å¸¸'
   
        print(f"\nğŸ“© è¨Šæ¯å…§å®¹ï¼š{sentence}")
        print(f"âœ… é æ¸¬çµæœï¼š{'è©é¨™' if label == 1 else 'æ­£å¸¸'}")
        print(f"ğŸ“Š ä¿¡å¿ƒå€¼ï¼š{round(prob*100, 2)}")
        print(f"âš ï¸ é¢¨éšªç­‰ç´šï¼š{risk}")
        return pre_label, prob, risk


def analyze_text(text):
    label, prob, risk = predict_single_sentence(model, tokenizer, text)
    return {
        "status": label,
        "confidence": round(prob*100, 2),  
        "suspicious_keywords": [risk]
    }

