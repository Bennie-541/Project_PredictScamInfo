
# å¼•å…¥é‡è¦å¥—ä»¶Import Library
# PyTorch ä¸»æ¨¡çµ„ï¼Œå’ŒTensorflowå¾ˆåƒ 
# å…±é€šé»ï¼šéƒ½æ˜¯æ·±åº¦å­¸ç¿’æ¡†æ¶ï¼Œæ”¯æ´å»ºæ§‹ç¥ç¶“ç¶²è·¯ã€è¨“ç·´èˆ‡æ¨è«–ï¼Œéƒ½æ”¯æ´GPUåŠ é€Ÿã€è¼‰å…¥æ¨¡å‹ï¼Œå’Œè™•ç†tensorç­‰ã€‚
# æ“ä½œæ¯”è¼ƒç›´è¦ºï¼Œæ¥è¿‘Pythonæœ¬èº«çš„é¢¨æ ¼ï¼Œå‹•æ…‹åœ–æ¶æ§‹(æ¯ä¸€æ¬¡forwardéƒ½å³æ™‚è¨ˆç®—)ï¼Œæ›´å®¹æ˜“é™¤éŒ¯ã€å¿«é€Ÿè¿­ä»£ï¼Œåœ¨ç ”ç©¶é ˜åŸŸéå¸¸æµè¡Œã€‚
# reæ˜¯Pythonå…§å»ºçš„æ­£å‰‡è¡¨ç¤ºå¼(regular expression)æ¨¡çµ„ï¼Œåœ¨é€™å°ˆæ¡ˆä¸­ç”¨ä¾†"ç”¨é—œéµè¦å‰‡ç¯©é¸æ–‡å­—å…§å®¹"ã€‚
# requestsæ˜¯ä¸€å€‹éå¸¸å¥½ç”¨çš„ HTTP è«‹æ±‚å¥—ä»¶ï¼Œèƒ½è®“ä½ å¾Pythonç™¼é€GET/POSTè«‹æ±‚ï¼Œåœ¨å°ˆæ¡ˆä¸­ç”¨ä¾†å¾Google Driveä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ(model.pth)ã€‚
# BertTokenizer:å¾Hugging Faceçš„transformerså¥—ä»¶è¼‰å…¥ä¸€å€‹å°ˆç”¨çš„ã€Œåˆ†è©å™¨ï¼ˆTokenizerï¼‰ã€ã€‚
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


import torch                
import re
import easyocr
import io
import numpy as np

from PIL import Image
from huggingface_hub import hf_hub_download
from transformers import BertTokenizer




# è¨­å®šè£ç½®ï¼ˆGPU å„ªå…ˆï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# é è¨­æ¨¡å‹èˆ‡ tokenizer ç‚º Noneï¼Œç›´åˆ°é¦–æ¬¡è«‹æ±‚æ‰è¼‰å…¥ï¼ˆå»¶é²è¼‰å…¥ï¼‰
model = None
tokenizer = None
# âœ… å»¶é²è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer
def load_model_and_tokenizer():
    global model, tokenizer
    if os.path.exists("model.pth"):
        model_path = "model.pth"
    else:
        model_path = hf_hub_download(repo_id="Bennie12/Bert-Lstm-Cnn-ScamDetecter", filename="model.pth")
    # åŒ¯å…¥æ¨¡å‹æ¶æ§‹ï¼ˆé¿å…åœ¨æ¨¡çµ„åˆå§‹åŒ–éšæ®µå°±å ç”¨å¤§é‡è¨˜æ†¶é«”ï¼‰
    from Backend.AI_Model_architecture import BertLSTM_CNN_Classifier
    """
      file_id = "19t6NlRFMc1i8bGtngRwIRtRcCmibdP9q"
    
    url = f"https://drive.google.com/uc?export=download&id={file_id}"  
    if not os.path.exists(model_path):   # å¦‚æœæœ¬åœ°é‚„æ²’æœ‰é€™å€‹æª”æ¡ˆ â†’ æ‰ä¸‹è¼‰ï¼ˆé¿å…é‡è¤‡ï¼‰
            print("ğŸ“¥ Downloading model from Google Drive...")
            r = requests.get(url)             # ç”¨requestsç™¼é€GETè«‹æ±‚åˆ°Google Drive
            with open(model_path, 'wb')as f: # æŠŠä¸‹è¼‰çš„æª”æ¡ˆå…§å®¹å¯«å…¥åˆ° model.pth æœ¬åœ°æª”æ¡ˆ
                f.write(r.content)
                print("âœ… Model downloaded.")     
    else:
            print("ğŸ“¦ Model already exists.")
    """
    # è¼‰å…¥æ¨¡å‹æ¶æ§‹èˆ‡åƒæ•¸ï¼Œåˆå§‹åŒ–æ¨¡å‹æ¶æ§‹ä¸¦è¼‰å…¥è¨“ç·´æ¬Šé‡
    model = BertLSTM_CNN_Classifier()
    
    # é€™è¡Œçš„åŠŸèƒ½æ˜¯ï¼šã€Œå¾ model_pathæŠŠ.pth æ¬Šé‡æª”æ¡ˆè®€é€²ä¾†ï¼Œè¼‰å…¥é€²æ¨¡å‹è£¡ã€ã€‚
    # model.load_state_dict(...)æŠŠä¸Šé¢è¼‰å…¥çš„æ¬Šé‡ã€Œå¥—é€²æ¨¡å‹æ¶æ§‹è£¡ã€
    # torch.load(...)è¼‰å…¥.pth æ¬Šé‡æª”æ¡ˆï¼Œæœƒè®Šæˆä¸€ä»½ Python å­—å…¸
    # map_location=deviceæŒ‡å®šæ¨¡å‹è¼‰å…¥åˆ° CPU é‚„æ˜¯ GPUï¼Œé¿å…å ±éŒ¯
    model.load_state_dict(torch.load(model_path, map_location=device))
    
    model.to(device)
    
    # é€™æ˜¯PyTorchä¸­çš„ã€Œæ¨è«–æ¨¡å¼ã€è¨­å®š
    # model.eval()æ¨¡å‹è™•æ–¼æ¨è«–ç‹€æ…‹ï¼ˆé—œæ‰ Dropout ç­‰éš¨æ©Ÿæ“ä½œï¼‰
    # åªè¦æ˜¯ç”¨ä¾†ã€Œé æ¸¬ã€è€Œä¸æ˜¯è¨“ç·´ï¼Œä¸€å®šè¦åŠ  .eval()ï¼
    model.eval()

    # åˆå§‹åŒ– tokenizer(ä¸è¦å¾ build_bert_inputs ä¸­å–)
    # è¼‰å…¥é è¨“ç·´å¥½çš„CKIPä¸­æ–‡BERTåˆ†è©å™¨
    # èƒ½æŠŠä¸­æ–‡å¥å­è½‰æˆ BERT æ¨¡å‹éœ€è¦çš„ input æ ¼å¼ï¼ˆinput_ids, attention_mask, token_type_idsï¼‰
    tokenizer = BertTokenizer.from_pretrained("ckiplab/bert-base-chinese")

    return model, tokenizer

all_preds = []
all_labels = []

# é æ¸¬å–®ä¸€å¥å­çš„åˆ†é¡çµæœï¼ˆè©é¨™ or æ­£å¸¸ï¼‰
# model: è¨“ç·´å¥½çš„PyTorchæ¨¡å‹
# tokenizer: åˆ†è©å™¨ï¼Œè² è²¬æŠŠä¸­æ–‡è½‰æˆ BERT èƒ½è™•ç†çš„æ•¸å€¼æ ¼å¼
# sentence: ä½¿ç”¨è€…è¼¸å…¥çš„æ–‡å­—å¥å­
# max_len: é™åˆ¶æœ€å¤§è¼¸å…¥é•·åº¦ï¼ˆé è¨­ 256 å€‹ tokenï¼‰
def predict_single_sentence(model, tokenizer, sentence, max_len=256):
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ä½¿ç”¨ with torch.no_grad()ï¼Œä»£è¡¨é€™æ®µç¨‹å¼ã€Œä¸éœ€è¦è¨˜éŒ„æ¢¯åº¦ã€
    # é€™æ¨£å¯ä»¥åŠ é€Ÿæ¨è«–ä¸¦ç¯€çœè¨˜æ†¶é«”
    with torch.no_grad():
         # ----------- æ–‡å­—å‰è™•ç†ï¼šæ¸…æ´—è¼¸å…¥å¥å­ -----------
        sentence = re.sub(r"\s+", "", sentence)  # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—å…ƒï¼ˆç©ºæ ¼ã€æ›è¡Œç­‰ï¼‰
        sentence = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚ï¼Œï¼ï¼Ÿ]", "", sentence)
        # ä¿ç•™å¸¸è¦‹ä¸­æ–‡å­—ã€è‹±æ•¸å­—èˆ‡æ¨™é»ç¬¦è™Ÿï¼Œå…¶ä»–å¥‡æ€ªç¬¦è™Ÿéƒ½ç§»é™¤
        # ----------- ä½¿ç”¨ BERT Tokenizer å°‡å¥å­ç·¨ç¢¼ -----------
        encoded = tokenizer(sentence,
                            return_tensors="pt",       # å›å‚³ PyTorch tensor æ ¼å¼ï¼ˆé è¨­æ˜¯ numpy æˆ– listï¼‰
                            truncation=True,           # è¶…éæœ€å¤§é•·åº¦å°±æˆªæ–·
                            padding="max_length",      # ä¸è¶³æœ€å¤§é•·åº¦å‰‡è£œç©ºç™½ï¼ˆPAD tokenï¼‰
                            max_length=max_len)        # è¨­å®šæœ€å¤§é•·åº¦ç‚º 256
        # æŠŠ tokenizer å›å‚³çš„è³‡æ–™é€é€²æ¨¡å‹å‰ï¼Œto(device)è½‰åˆ°æŒ‡å®šçš„è£ç½®ï¼ˆGPU or CPUï¼‰
        input_ids = encoded["input_ids"].to(device)
        attention_mask = encoded["attention_mask"].to(device)
        token_type_ids = encoded["token_type_ids"].to(device)
        # ----------- æ¨¡å‹æ¨è«–ï¼šè¼¸å‡ºè©é¨™çš„æ©Ÿç‡å€¼ -----------
        output = model(input_ids, attention_mask, token_type_ids)# å›å‚³çš„æ˜¯ä¸€å€‹æ©Ÿç‡å€¼ï¼ˆfloatï¼‰
        prob = output.item()  # å¾ tensor å–å‡ºç´”æ•¸å­—ï¼Œä¾‹å¦‚ 0.86
        label = int(prob > 0.5)  # å¦‚æœæ©Ÿç‡ > 0.5ï¼Œæ¨™ç‚ºã€Œè©é¨™ã€ï¼ˆ1ï¼‰ï¼Œå¦å‰‡ç‚ºã€Œæ­£å¸¸ã€ï¼ˆ0ï¼‰
        # ----------- æ ¹æ“šæ©Ÿç‡é€²è¡Œé¢¨éšªåˆ†ç´š -----------
        if prob > 0.9:
            risk = "ğŸ”´ é«˜é¢¨éšªï¼ˆæ¥µå¯èƒ½æ˜¯è©é¨™ï¼‰"
        elif prob > 0.5:
            risk = "ğŸŸ¡ ä¸­é¢¨éšªï¼ˆå¯ç–‘ï¼‰"
        else:
            risk = "ğŸŸ¢ ä½é¢¨éšªï¼ˆæ­£å¸¸ï¼‰"
        # ----------- æ ¹æ“š label åˆ¤æ–·æ–‡å­—çµæœ -----------
        pre_label ='è©é¨™'if label == 1 else 'æ­£å¸¸'
        # ----------- é¡¯ç¤ºæ¨è«–è³‡è¨Šï¼ˆå¾Œç«¯çµ‚ç«¯æ©Ÿï¼‰ -----------
        print(f"\nğŸ“© è¨Šæ¯å…§å®¹ï¼š{sentence}")
        print(f"âœ… é æ¸¬çµæœï¼š{'è©é¨™' if label == 1 else 'æ­£å¸¸'}")
        print(f"ğŸ“Š ä¿¡å¿ƒå€¼ï¼š{round(prob*100, 2)}")
        print(f"âš ï¸ é¢¨éšªç­‰ç´šï¼š{risk}")
        # ----------- å›å‚³çµæœçµ¦å‘¼å«ç«¯ï¼ˆé€šå¸¸æ˜¯ APIï¼‰ -----------
        # çµ„æˆä¸€å€‹ Python å­—å…¸ï¼ˆå°æ‡‰ API çš„ JSON è¼¸å‡ºæ ¼å¼ï¼‰
        return {
        "label" : pre_label,                  # é æ¸¬åˆ†é¡ï¼ˆ"è©é¨™" or "æ­£å¸¸"ï¼‰
        "prob" : prob, # é æ¸¬åˆ†é¡ï¼ˆ"è©é¨™" or "æ­£å¸¸"ï¼‰  
        "risk" : risk     # ç”¨é¢¨éšªåˆ†ç´šç•¶ä½œ"å¯ç–‘æç¤º"æ”¾é€² listï¼ˆåç¨±ç‚º suspicious_keywordsï¼‰
    }

# analyze_text(text)å°æ‡‰app.pyç¬¬117è¡Œ
# é€™å€‹å‡½å¼æ˜¯ã€Œå°å¤–çš„ç°¡åŒ–ç‰ˆæœ¬ã€ï¼šè¼¸å…¥ä¸€å¥æ–‡å­— â†’ å›å‚³è©é¨™åˆ¤å®šçµæœ
# ç”¨åœ¨ä¸»ç¨‹å¼æˆ– FastAPI å¾Œç«¯ä¸­ï¼Œæ˜¯æ•´å€‹æ¨¡å‹é æ¸¬æµç¨‹çš„å…¥å£é»


#------------ CNN ------------
def extract_suspicious_tokens_cnn(model, tokenizer, text, top_k=3):
    model.eval()
    model.to(device)

    # æ¸…ç†èˆ‡ç·¨ç¢¼è¼¸å…¥æ–‡å­—
    sentence = re.sub(r"\s+", "", text)
    sentence = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚ï¼Œï¼ï¼Ÿ]", "", sentence)

    encoded = tokenizer(sentence,
                        return_tensors="pt",
                        truncation=True,
                        padding="max_length",
                        max_length=128)

    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)
    token_type_ids = encoded["token_type_ids"].to(device)

    # å‰å‘å‚³éç›´åˆ° CNN è¼¸å‡º
    with torch.no_grad():
        hidden_states = model.bert(input_ids=input_ids,
                                   attention_mask=attention_mask,
                                   token_type_ids=token_type_ids).last_hidden_state
        lstm_out, _ = model.LSTM(hidden_states)
        conv_input = lstm_out.transpose(1, 2)
        conv_out = model.conv1(conv_input)  # conv_out = [batch, 128, seq_len]

    # é€™è£¡æœƒå°‡conv_outçš„è¼¸å‡º[batch, 128, seq_len]ï¼Œå£“ç¸®æˆ[seq_len]ï¼Œä¹Ÿå°±æ˜¯è½‰æ›æˆbertç·¨ç¢¼å½¢å‹¢çš„å¥å­ã€‚
    token_scores = conv_out.mean(dim=1).squeeze()

    # torch.topk(token_scores, top_k)æœƒå¾—åˆ°åˆ†æ•¸é«˜çš„tokenï¼Œå’Œå°æ‡‰ç´¢å¼•ä½ç½®ï¼Œ.indicesåªç•™ä¸‹ç´¢å¼•ï¼Œ.cpu()æŠŠçµæœå¾GPUç§»åˆ°CPUï¼ˆå¿…è¦æ‰èƒ½è½‰ç‚º listï¼‰ï¼Œ
    # .tolist()è½‰åŒ–æˆlistæ ¼å¼ã€‚æŒ‘å‡ºé‡è¦æ€§æœ€é«˜çš„å¹¾å€‹ token çš„ä½ç½®ç´¢å¼•ã€‚
    topk_indices = torch.topk(token_scores, top_k).indices.cpu().tolist()

    """ 
    tokenizer.convert_ids_to_tokens(input_ids.squeeze())å°‡bertç·¨ç¢¼é‚„åŸæˆåŸå§‹æ–‡å­—
    é€™æ®µinput_ids = encoded["input_ids"].to(device)è¼¸å‡ºçš„ç·¨ç¢¼ï¼Œé‚„åŸæˆæ–‡å­—
    .squeeze() å»æ‰ batch ç¶­åº¦ï¼Œå¾—åˆ° [seq_len]ã€‚
    [tokens[i] for i in topk_indices if tokens[i] not in ["[PAD]", "[CLS]", "[SEP]"]]
    ä¸Šé¢çš„ç¨‹å¼ç¢¼ç‚ºï¼Œiç‚ºtopk_indicesæŒ‘å‡ºçš„ç´¢å¼•ï¼Œtoken[i]ç‚ºåˆ†æ•¸æœ€é«˜çš„æ–‡å­—ï¼Œä¹Ÿå°±æ˜¯å¯ç–‘çš„è©å¥ã€‚
    not in å°±èƒ½é¿å…é¸åˆ°å°±èƒ½é¿å…é¸åˆ°[CLS]ã€[SEP]ã€ [PAD]
    [CLS] é–‹å§‹ç¬¦è™Ÿ = 101
    [SEP] çµæŸç¬¦è™Ÿ = 102
    [PAD] è£œç©ºç™½ = 0
    """
    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())
    suspicious_tokens = [tokens[i] for i in topk_indices if tokens[i] not in ["[PAD]", "[CLS]", "[SEP]"]]

    return suspicious_tokens


#------------ Bert Attention ------------
def extract_suspicious_tokens_attention(model, tokenizer, text, top_k=3):
    from transformers import BertModel  # é¿å…é‡è¤‡ import

    sentence = re.sub(r"\s+", "", text)
    sentence = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚ï¼Œï¼ï¼Ÿ]", "", sentence)

    encoded = tokenizer(sentence,
                        return_tensors="pt",
                        truncation=True,
                        padding="max_length",
                        max_length=128)

    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)
    token_type_ids = encoded["token_type_ids"].to(device)

    with torch.no_grad():
        bert_outputs = model.bert(input_ids=input_ids,
                                  attention_mask=attention_mask,
                                  token_type_ids=token_type_ids,
                                  output_attentions=True)
        # å–ç¬¬ä¸€å±¤ç¬¬0å€‹ head çš„ attentionï¼ˆCLS â†’ all tokensï¼‰
        """
        attentions[0]ç¬¬ 0 å±¤ attentionï¼ˆBERT ç¬¬ 1 å±¤ï¼‰ï¼Œ[0, 0, 0, :]å–å‡ºç¬¬ 0 å€‹ batchã€ç¬¬ 0 å€‹ headã€ç¬¬ 0 å€‹ tokenï¼ˆCLSï¼‰å°æ‰€æœ‰ token çš„æ³¨æ„åŠ›åˆ†æ•¸
        
        """
        attention_scores = bert_outputs.attentions[0][0, 0, 0, :]  # [seq_len]
    
    topk_indices = torch.topk(attention_scores, top_k).indices.cpu().tolist()
    
    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())
    suspicious_tokens = [tokens[i] for i in topk_indices if tokens[i] not in ["[PAD]", "[CLS]", "[SEP]"]]

    return suspicious_tokens



def analyze_text(text, explain_mode="cnn"):
    model, tokenizer = load_model_and_tokenizer()
    model.eval()

    # é æ¸¬æ¨™ç±¤èˆ‡ä¿¡å¿ƒåˆ†æ•¸
    result = predict_single_sentence(model, tokenizer, text)
    label = result["label"]
    prob = result["prob"]
    risk = result["risk"]
    # æ ¹æ“šæ¨¡å¼æ“·å–å¯ç–‘è©
    if explain_mode == "cnn":
        suspicious = extract_suspicious_tokens_cnn(model, tokenizer, text)
    elif explain_mode == "bert":
        suspicious = extract_suspicious_tokens_attention(model, tokenizer, text)
    elif explain_mode == "both":
        cnn_tokens = extract_suspicious_tokens_cnn(model, tokenizer, text)
        bert_tokens = extract_suspicious_tokens_attention(model, tokenizer, text)
        suspicious = list(set(cnn_tokens + bert_tokens))

    return {
        "status": label,
        "confidence": round(prob * 100, 2),
        "suspicious_keywords": [str(s) for s in suspicious]
    }

def analyze_image(file_bytes, explain_mode = "cnn"):
    image = Image.open(io.BytesIO(file_bytes))
    image_np = np.array(image)
    reader = easyocr.Reader(['ch_tra', 'en'], gpu=torch.cuda.is_available())
    results = reader.readtext(image_np)
    
    text = ' '.join([res[1] for res in results]).strip()
    
    if not text:
        return{
            "status" : "ç„¡æ³•è¾¨è­˜æ–‡å­—",
            "confidence" : 0.0,
            "suspicious_keywords" : ["åœ–ç‰‡ä¸­ç„¡å¯è¾¨è­˜çš„ä¸­æ–‡è‹±æ–‡"]
        }
    return analyze_text(text, explain_mode=explain_mode)