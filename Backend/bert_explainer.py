#pip install transformers
#pip install torch         //transformers å¥—ä»¶éœ€è¦
#pip install scikit-learn
#pip install transformers torch

# å¼•å…¥é‡è¦å¥—ä»¶Import Library
# PyTorch ä¸»æ¨¡çµ„ï¼Œå’ŒTensorflowå¾ˆåƒ 
# å…±é€šé»ï¼šéƒ½æ˜¯æ·±åº¦å­¸ç¿’æ¡†æ¶ï¼Œæ”¯æ´å»ºæ§‹ç¥ç¶“ç¶²è·¯ã€è¨“ç·´èˆ‡æ¨è«–ï¼Œéƒ½æ”¯æ´GPUåŠ é€Ÿã€è¼‰å…¥æ¨¡å‹ï¼Œå’Œè™•ç†tensorç­‰ã€‚
# æ“ä½œæ¯”è¼ƒç›´è¦ºï¼Œæ¥è¿‘Pythonæœ¬èº«çš„é¢¨æ ¼ï¼Œå‹•æ…‹åœ–æ¶æ§‹(æ¯ä¸€æ¬¡forwardéƒ½å³æ™‚è¨ˆç®—)ï¼Œæ›´å®¹æ˜“é™¤éŒ¯ã€å¿«é€Ÿè¿­ä»£ï¼Œåœ¨ç ”ç©¶é ˜åŸŸéå¸¸æµè¡Œã€‚
# reæ˜¯Pythonå…§å»ºçš„æ­£å‰‡è¡¨ç¤ºå¼(regular expression)æ¨¡çµ„ï¼Œåœ¨é€™å°ˆæ¡ˆä¸­ç”¨ä¾†"ç”¨é—œéµè¦å‰‡ç¯©é¸æ–‡å­—å…§å®¹"ã€‚
# requestsæ˜¯ä¸€å€‹éå¸¸å¥½ç”¨çš„ HTTP è«‹æ±‚å¥—ä»¶ï¼Œèƒ½è®“ä½ å¾Pythonç™¼é€GET/POSTè«‹æ±‚ï¼Œåœ¨å°ˆæ¡ˆä¸­ç”¨ä¾†å¾Google Driveä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ(model.pth)ã€‚
# BertTokenizer:å¾Hugging Faceçš„transformerså¥—ä»¶è¼‰å…¥ä¸€å€‹å°ˆç”¨çš„ã€Œåˆ†è©å™¨ï¼ˆTokenizerï¼‰ã€ã€‚
import torch                
import re
import os
import requests

from transformers import BertTokenizer

# é è¨­æ¨¡å‹èˆ‡ tokenizer ç‚º Noneï¼Œç›´åˆ°é¦–æ¬¡è«‹æ±‚æ‰è¼‰å…¥ï¼ˆå»¶é²è¼‰å…¥ï¼‰
model = None
tokenizer = None
# âœ… å»¶é²è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer
def load_model_and_tokenizer():
    global model, tokenizer

    # å¦‚æœå·²ç¶“è¼‰å…¥ï¼Œå°±ç›´æ¥å›å‚³ï¼Œä¸é‡è¤‡è¼‰å…¥
    if model is not None and tokenizer is not None:
        return model, tokenizer

    # åŒ¯å…¥æ¨¡å‹æ¶æ§‹ï¼ˆé¿å…åœ¨æ¨¡çµ„åˆå§‹åŒ–éšæ®µå°±å ç”¨å¤§é‡è¨˜æ†¶é«”ï¼‰
    from Backend.AI_Model_architecture import BertLSTM_CNN_Classifier
    
    # åœ¨ä½  load æ¨¡å‹ä¹‹å‰åŸ·è¡Œ
    # é€™è¡Œæ˜¯ç‚ºäº†å–å¾—æ¨¡å‹å„²å­˜è·¯å¾‘ï¼Œç¢ºä¿å³ä½¿æ›ä¸åŒé›»è…¦æˆ–ä¸Šé›²éƒ¨ç½²ä¹Ÿèƒ½æ­£ç¢ºæ‰¾åˆ°æª”æ¡ˆã€‚
    # ç°¡å–®è¬›ï¼šé€™è¡Œå°±æ˜¯ã€Œæ‰¾åˆ°é€™æ”¯ç¨‹å¼æ‰€åœ¨çš„è³‡æ–™å¤¾ï¼Œä¸¦æŒ‡å®šé‚£è£¡çš„ model.pth æª”æ¡ˆã€
    # os.path.join(è³‡æ–™å¤¾,"model.pth")æŠŠè³‡æ–™å¤¾+æª”åã€Œå®‰å…¨åœ°åˆä½µã€ï¼Œè®Šæˆå®Œæ•´è·¯å¾‘(è·¨å¹³å°å…¼å®¹)
    # __file__æ˜¯Pythonå…§å»ºè®Šæ•¸ï¼Œä»£è¡¨ã€Œç›®å‰é€™æ”¯ç¨‹å¼ç¢¼çš„æª”æ¡ˆè·¯å¾‘ã€
    # os.path.dirname(__file__)å–å¾—é€™æ”¯.py æª”çš„ã€Œè³‡æ–™å¤¾è·¯å¾‘ã€
    model_path = os.path.join(os.path.dirname(__file__), "model.pth")
    file_id = "19t6NlRFMc1i8bGtngRwIRtRcCmibdP9q"
    
    download_model_from_Gdrive(file_id, model_path)

    # Google Drive æ¨¡å‹æª”æ¡ˆ ID èˆ‡å„²å­˜è·¯å¾‘
    # é€™æ˜¯ä¸€å€‹å‡½å¼ï¼Œè®“ä½ è‡ªå‹•å¾ Google Drive æŠ“ä¸‹æ¨¡å‹æª”æ¡ˆ .pthã€‚
    # file_id: ä½ å¾ Google Drive æ‹¿åˆ°çš„æ¨¡å‹ ID
    # destination: ä½ è¦å„²å­˜çš„æœ¬åœ°æª”æ¡ˆè·¯å¾‘ï¼ˆä¾‹å¦‚ "model.pth"ï¼‰
    def download_model_from_Gdrive(file_id, destination):#Model.pthé€£çµ(https://drive.google.com/file/d/19t6NlRFMc1i8bGtngRwIRtRcCmibdP9q/view?usp=drive_link)
        # urlè®Šæ•¸ç”¨ä¾†ç”¢ç”ŸçœŸæ­£çš„ã€Œç›´æ¥ä¸‹è¼‰é€£çµã€file_idé è¨­æ˜¯Googleé›²ç«¯è£¡çš„model.pthæª”æ¡ˆidï¼Œè®“ç¨‹å¼èƒ½ä¸‹è¼‰model.pth
        url = f"https://drive.google.com/uc?export=download&id={file_id}"  
        if not os.path.exists(destination):   # å¦‚æœæœ¬åœ°é‚„æ²’æœ‰é€™å€‹æª”æ¡ˆ â†’ æ‰ä¸‹è¼‰ï¼ˆé¿å…é‡è¤‡ï¼‰
            print("ğŸ“¥ Downloading model from Google Drive...")
            r = requests.get(url)             # ç”¨requestsç™¼é€GETè«‹æ±‚åˆ°Google Drive
            with open(destination, 'wb')as f: # æŠŠä¸‹è¼‰çš„æª”æ¡ˆå…§å®¹å¯«å…¥åˆ° model.pth æœ¬åœ°æª”æ¡ˆ
                f.write(r.content)
                print("âœ… Model downloaded.")     
        else:
            print("ğŸ“¦ Model already exists.")

    # è¨­å®šè£ç½®ï¼ˆGPU å„ªå…ˆï¼‰
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # è¼‰å…¥æ¨¡å‹æ¶æ§‹èˆ‡åƒæ•¸ï¼Œåˆå§‹åŒ–æ¨¡å‹æ¶æ§‹ä¸¦è¼‰å…¥è¨“ç·´æ¬Šé‡
    model = BertLSTM_CNN_Classifier()
    
    # é€™è¡Œçš„åŠŸèƒ½æ˜¯ï¼šã€Œå¾ model_pathæŠŠ.pth æ¬Šé‡æª”æ¡ˆè®€é€²ä¾†ï¼Œè¼‰å…¥é€²æ¨¡å‹è£¡ã€ã€‚
    # model.load_state_dict(...)æŠŠä¸Šé¢è¼‰å…¥çš„æ¬Šé‡ã€Œå¥—é€²æ¨¡å‹æ¶æ§‹è£¡ã€
    # torch.load(...)è¼‰å…¥.pth æ¬Šé‡æª”æ¡ˆï¼Œæœƒè®Šæˆä¸€ä»½ Python å­—å…¸
    # map_location=deviceæŒ‡å®šæ¨¡å‹è¼‰å…¥åˆ° CPU é‚„æ˜¯ GPUï¼Œé¿å…å ±éŒ¯
    model.load_state_dict(torch.load(model_path, map_location=device))
    
    model.to(device)
    
    # é€™æ˜¯PyTorchä¸­çš„ã€Œæ¨è«–æ¨¡å¼ã€è¨­å®š
    # model.eval()æ¨¡å‹è™•æ–¼æ¨è«–ç‹€æ…‹ï¼ˆé—œæ‰ Dropout ç­‰éš¨æ©Ÿæ“ä½œï¼‰
    # åªè¦æ˜¯ç”¨ä¾†ã€Œé æ¸¬ã€è€Œä¸æ˜¯è¨“ç·´ï¼Œä¸€å®šè¦åŠ  .eval()ï¼
    model.eval()

    # åˆå§‹åŒ– tokenizer(ä¸è¦å¾ build_bert_inputs ä¸­å–)
    # è¼‰å…¥é è¨“ç·´å¥½çš„CKIPä¸­æ–‡BERTåˆ†è©å™¨
    # èƒ½æŠŠä¸­æ–‡å¥å­è½‰æˆ BERT æ¨¡å‹éœ€è¦çš„ input æ ¼å¼ï¼ˆinput_ids, attention_mask, token_type_idsï¼‰
    tokenizer = BertTokenizer.from_pretrained("ckiplab/bert-base-chinese")

    return model, tokenizer

all_preds = []
all_labels = []

# é æ¸¬å–®ä¸€å¥å­çš„åˆ†é¡çµæœï¼ˆè©é¨™ or æ­£å¸¸ï¼‰
# model: è¨“ç·´å¥½çš„PyTorchæ¨¡å‹
# tokenizer: åˆ†è©å™¨ï¼Œè² è²¬æŠŠä¸­æ–‡è½‰æˆ BERT èƒ½è™•ç†çš„æ•¸å€¼æ ¼å¼
# sentence: ä½¿ç”¨è€…è¼¸å…¥çš„æ–‡å­—å¥å­
# max_len: é™åˆ¶æœ€å¤§è¼¸å…¥é•·åº¦ï¼ˆé è¨­ 256 å€‹ tokenï¼‰
def predict_single_sentence(model, tokenizer, sentence, max_len=256):
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ä½¿ç”¨ with torch.no_grad()ï¼Œä»£è¡¨é€™æ®µç¨‹å¼ã€Œä¸éœ€è¦è¨˜éŒ„æ¢¯åº¦ã€
    # é€™æ¨£å¯ä»¥åŠ é€Ÿæ¨è«–ä¸¦ç¯€çœè¨˜æ†¶é«”
    with torch.no_grad():
         # ----------- æ–‡å­—å‰è™•ç†ï¼šæ¸…æ´—è¼¸å…¥å¥å­ -----------
        sentence = re.sub(r"\s+", "", sentence)  # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—å…ƒï¼ˆç©ºæ ¼ã€æ›è¡Œç­‰ï¼‰
        sentence = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9ã€‚ï¼Œï¼ï¼Ÿ]", "", sentence)
        # ä¿ç•™å¸¸è¦‹ä¸­æ–‡å­—ã€è‹±æ•¸å­—èˆ‡æ¨™é»ç¬¦è™Ÿï¼Œå…¶ä»–å¥‡æ€ªç¬¦è™Ÿéƒ½ç§»é™¤
        # ----------- ä½¿ç”¨ BERT Tokenizer å°‡å¥å­ç·¨ç¢¼ -----------
        encoded = tokenizer(sentence,
                            return_tensors="pt",       # å›å‚³ PyTorch tensor æ ¼å¼ï¼ˆé è¨­æ˜¯ numpy æˆ– listï¼‰
                            truncation=True,           # è¶…éæœ€å¤§é•·åº¦å°±æˆªæ–·
                            padding="max_length",      # ä¸è¶³æœ€å¤§é•·åº¦å‰‡è£œç©ºç™½ï¼ˆPAD tokenï¼‰
                            max_length=max_len)        # è¨­å®šæœ€å¤§é•·åº¦ç‚º 256
        # æŠŠ tokenizer å›å‚³çš„è³‡æ–™é€é€²æ¨¡å‹å‰ï¼Œto(device)è½‰åˆ°æŒ‡å®šçš„è£ç½®ï¼ˆGPU or CPUï¼‰
        input_ids = encoded["input_ids"].to(device)
        attention_mask = encoded["attention_mask"].to(device)
        token_type_ids = encoded["token_type_ids"].to(device)
        # ----------- æ¨¡å‹æ¨è«–ï¼šè¼¸å‡ºè©é¨™çš„æ©Ÿç‡å€¼ -----------
        output = model.forward(input_ids, attention_mask, token_type_ids)# å›å‚³çš„æ˜¯ä¸€å€‹æ©Ÿç‡å€¼ï¼ˆfloatï¼‰
        prob = output.item()  # å¾ tensor å–å‡ºç´”æ•¸å­—ï¼Œä¾‹å¦‚ 0.86
        label = int(prob > 0.5)  # å¦‚æœæ©Ÿç‡ > 0.5ï¼Œæ¨™ç‚ºã€Œè©é¨™ã€ï¼ˆ1ï¼‰ï¼Œå¦å‰‡ç‚ºã€Œæ­£å¸¸ã€ï¼ˆ0ï¼‰
        # ----------- æ ¹æ“šæ©Ÿç‡é€²è¡Œé¢¨éšªåˆ†ç´š -----------
        if prob > 0.9:
            risk = "ğŸ”´ é«˜é¢¨éšªï¼ˆæ¥µå¯èƒ½æ˜¯è©é¨™ï¼‰"
        elif prob > 0.5:
            risk = "ğŸŸ¡ ä¸­é¢¨éšªï¼ˆå¯ç–‘ï¼‰"
        else:
            risk = "ğŸŸ¢ ä½é¢¨éšªï¼ˆæ­£å¸¸ï¼‰"
        # ----------- æ ¹æ“š label åˆ¤æ–·æ–‡å­—çµæœ -----------
        pre_label ='è©é¨™'if label == 1 else 'æ­£å¸¸'
        # ----------- é¡¯ç¤ºæ¨è«–è³‡è¨Šï¼ˆå¾Œç«¯çµ‚ç«¯æ©Ÿï¼‰ -----------
        print(f"\nğŸ“© è¨Šæ¯å…§å®¹ï¼š{sentence}")
        print(f"âœ… é æ¸¬çµæœï¼š{'è©é¨™' if label == 1 else 'æ­£å¸¸'}")
        print(f"ğŸ“Š ä¿¡å¿ƒå€¼ï¼š{round(prob*100, 2)}")
        print(f"âš ï¸ é¢¨éšªç­‰ç´šï¼š{risk}")
        # ----------- å›å‚³çµæœçµ¦å‘¼å«ç«¯ï¼ˆé€šå¸¸æ˜¯ APIï¼‰ -----------
        # çµ„æˆä¸€å€‹ Python å­—å…¸ï¼ˆå°æ‡‰ API çš„ JSON è¼¸å‡ºæ ¼å¼ï¼‰
        return {
        "status": pre_label,                  # é æ¸¬åˆ†é¡ï¼ˆ"è©é¨™" or "æ­£å¸¸"ï¼‰
        "confidence": round(prob*100, 2), # é æ¸¬åˆ†é¡ï¼ˆ"è©é¨™" or "æ­£å¸¸"ï¼‰  
        "suspicious_keywords": [risk]     # ç”¨é¢¨éšªåˆ†ç´šç•¶ä½œ"å¯ç–‘æç¤º"æ”¾é€² listï¼ˆåç¨±ç‚º suspicious_keywordsï¼‰
    }

# analyze_text(text)å°æ‡‰app.pyç¬¬117è¡Œ
# é€™å€‹å‡½å¼æ˜¯ã€Œå°å¤–çš„ç°¡åŒ–ç‰ˆæœ¬ã€ï¼šè¼¸å…¥ä¸€å¥æ–‡å­— â†’ å›å‚³è©é¨™åˆ¤å®šçµæœ
# ç”¨åœ¨ä¸»ç¨‹å¼æˆ– FastAPI å¾Œç«¯ä¸­ï¼Œæ˜¯æ•´å€‹æ¨¡å‹é æ¸¬æµç¨‹çš„å…¥å£é»

def analyze_text(text):
    # å‘¼å«å‰é¢å®šç¾©å¥½çš„ predict_single_sentence()
    # å‚³å…¥æ¨¡å‹ã€tokenizerã€è¼¸å…¥æ–‡å­— â†’ å›å‚³ä¸‰é …çµæœ
    model, tokenizer = load_model_and_tokenizer()
    return predict_single_sentence(model, tokenizer, text)
    
    

